{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Dagger - 0.1.0 Dagger is a distributed, scalable, durable, and highly available orchestration engine to execute asynchronous and synchronous long-running business logic in a scalable and resilient way. Dagger requires Python 3.7 or later for the new async/await _ syntax, and variable type annotations. Here's an example of how to use the library to build and run a workflow: import logging from uuid import uuid1 from dagger.service.services import Dagger from dagger.modeler.definition import * from dagger.templates.template import * logger = logging . getLogger ( __name__ ) workflow_engine = Dagger ( broker = \"kafka://localhost:9092\" , store = \"aerospike://\" , datadir = \"/tmp/data/\" ) @Dagger . register_template ( 'OrderWorkflow' ) def order_template ( template_name : str ) -> ITemplateDAG : # Create empty default Template template_builder = DefaultTemplateBuilder ( Dagger . app ) template_builder . set_name ( template_name ) template_builder . set_type ( DefaultTemplateDAGInstance ) # First process in template (first node in DAG) named waving payment_process_builder = ProcessTemplateDagBuilder ( Dagger . app ) payment_process_builder . set_name ( \"PAYMENT\" ) # First task and topic in payment process payment_command_topic = Dagger . create_topic ( \"PAYMENT_COMMAND\" , key_type = bytes , value_type = bytes ) payment_command_task_template_builder = KafkaCommandTaskTemplateBuilder ( Dagger . app ) payment_command_task_template_builder . set_topic ( payment_command_topic ) payment_command_task_template_builder . set_type ( PaymentKafkaCommandTask ) # Second task and topic in payment process payment_topic : Topic = Dagger . create_topic ( \"PAYMENT_LISTENER\" , key_type = bytes , value_type = bytes ) payment_listener_task_template_builder = KafkaListenerTaskTemplateBuilder ( Dagger . app ) payment_listener_task_template_builder . set_topic ( payment_topic ) payment_listener_task_template_builder . set_type ( PaymentKafkaListenerTask ) # Link and build tasks in payment process (define root task and order, essentially just created a child DAG inside the parent DAG) payment_listener_task_template = payment_listener_task_template_builder . build () payment_command_task_template_builder . set_next ( payment_listener_task_template ) payment_command_task_template = payment_command_task_template_builder . build () payment_process_builder . set_root_task ( payment_command_task_template ) payment_process_builder . set_type ( DefaultProcessTemplateDAGInstance ) # Build more processes like above and link them [ ... ] # Link and build processes in DAG (define root task and order) Assuming one more process called \"SHIPPING\" was created, this would be the flow: shipping_template = shipping_process_builder . build () payment_process_builder . set_next_process ( shipping_template ) payment_template = payment_process_builder . build () template_builder . set_root ( payment_template ) btemplate = template_builder . build () return btemplate # Starts the worker workflow_engine . main () The register_template decorator defines a \"DAG processor\" that essentially defines the various processes and child tasks the DAG executes. In the example above the code creates a named template OrderWorkflow and associates a PAYMENT process with 2 child tasks PAYMENT_LISTENER using PaymentKafkaListenerTask and PAYMENT_COMMAND using the PaymentKafkaCommandTask definition. The SHIPPING process follows after the PAYMENT process with similarly named topics and processes and the template defines the root process and links them in a DAG (Directed Acyclic Graph) structure The application can define as many DAG'S it needs to model using the register_template decorator. dagger populates all the DAG templates in the codebase decorated with register_template Here's and example of how to create an instance of a specific DAG: template = workflow_engine . get_template ( 'OrderWorkflow' ) runtime_parameters : Dict [ str , str ] = dict () runtime_parameters [ 'customer_name' ] = ` EXAMPLE_CUSTOMER ` runtime_parameters [ 'order_number' ] = 'EXAMPLE_ORDER' workflow_instance = await template . create_instance ( uuid1 (), runtime_parameters ) To begin execution of the DAG instance created above await workflow_engine . submit ( workflow_instance ) This begins the actual execution of the tasks created by the template definition and executes them in the sequence as defined in the template. Where to Start? To learn the basics of how to start using dagger , read the Getting Started page. Detailed Documentation To learn more about the various ways dagger can be used, read the Usage Guide page. API Reference To find detailed information about a specific function or class, read the API Reference .","title":"Overview"},{"location":"#dagger-010","text":"Dagger is a distributed, scalable, durable, and highly available orchestration engine to execute asynchronous and synchronous long-running business logic in a scalable and resilient way. Dagger requires Python 3.7 or later for the new async/await _ syntax, and variable type annotations. Here's an example of how to use the library to build and run a workflow: import logging from uuid import uuid1 from dagger.service.services import Dagger from dagger.modeler.definition import * from dagger.templates.template import * logger = logging . getLogger ( __name__ ) workflow_engine = Dagger ( broker = \"kafka://localhost:9092\" , store = \"aerospike://\" , datadir = \"/tmp/data/\" ) @Dagger . register_template ( 'OrderWorkflow' ) def order_template ( template_name : str ) -> ITemplateDAG : # Create empty default Template template_builder = DefaultTemplateBuilder ( Dagger . app ) template_builder . set_name ( template_name ) template_builder . set_type ( DefaultTemplateDAGInstance ) # First process in template (first node in DAG) named waving payment_process_builder = ProcessTemplateDagBuilder ( Dagger . app ) payment_process_builder . set_name ( \"PAYMENT\" ) # First task and topic in payment process payment_command_topic = Dagger . create_topic ( \"PAYMENT_COMMAND\" , key_type = bytes , value_type = bytes ) payment_command_task_template_builder = KafkaCommandTaskTemplateBuilder ( Dagger . app ) payment_command_task_template_builder . set_topic ( payment_command_topic ) payment_command_task_template_builder . set_type ( PaymentKafkaCommandTask ) # Second task and topic in payment process payment_topic : Topic = Dagger . create_topic ( \"PAYMENT_LISTENER\" , key_type = bytes , value_type = bytes ) payment_listener_task_template_builder = KafkaListenerTaskTemplateBuilder ( Dagger . app ) payment_listener_task_template_builder . set_topic ( payment_topic ) payment_listener_task_template_builder . set_type ( PaymentKafkaListenerTask ) # Link and build tasks in payment process (define root task and order, essentially just created a child DAG inside the parent DAG) payment_listener_task_template = payment_listener_task_template_builder . build () payment_command_task_template_builder . set_next ( payment_listener_task_template ) payment_command_task_template = payment_command_task_template_builder . build () payment_process_builder . set_root_task ( payment_command_task_template ) payment_process_builder . set_type ( DefaultProcessTemplateDAGInstance ) # Build more processes like above and link them [ ... ] # Link and build processes in DAG (define root task and order) Assuming one more process called \"SHIPPING\" was created, this would be the flow: shipping_template = shipping_process_builder . build () payment_process_builder . set_next_process ( shipping_template ) payment_template = payment_process_builder . build () template_builder . set_root ( payment_template ) btemplate = template_builder . build () return btemplate # Starts the worker workflow_engine . main () The register_template decorator defines a \"DAG processor\" that essentially defines the various processes and child tasks the DAG executes. In the example above the code creates a named template OrderWorkflow and associates a PAYMENT process with 2 child tasks PAYMENT_LISTENER using PaymentKafkaListenerTask and PAYMENT_COMMAND using the PaymentKafkaCommandTask definition. The SHIPPING process follows after the PAYMENT process with similarly named topics and processes and the template defines the root process and links them in a DAG (Directed Acyclic Graph) structure The application can define as many DAG'S it needs to model using the register_template decorator. dagger populates all the DAG templates in the codebase decorated with register_template Here's and example of how to create an instance of a specific DAG: template = workflow_engine . get_template ( 'OrderWorkflow' ) runtime_parameters : Dict [ str , str ] = dict () runtime_parameters [ 'customer_name' ] = ` EXAMPLE_CUSTOMER ` runtime_parameters [ 'order_number' ] = 'EXAMPLE_ORDER' workflow_instance = await template . create_instance ( uuid1 (), runtime_parameters ) To begin execution of the DAG instance created above await workflow_engine . submit ( workflow_instance ) This begins the actual execution of the tasks created by the template definition and executes them in the sequence as defined in the template.","title":"Dagger - 0.1.0"},{"location":"#where-to-start","text":"To learn the basics of how to start using dagger , read the Getting Started page.","title":"Where to Start?"},{"location":"#detailed-documentation","text":"To learn more about the various ways dagger can be used, read the Usage Guide page.","title":"Detailed Documentation"},{"location":"#api-reference","text":"To find detailed information about a specific function or class, read the API Reference .","title":"API Reference"},{"location":"api/","text":"API Documentation Dagger Bases: Service The Dagger class to create the workflow engine. __create_app () Initializes instance of Faust Returns: Type Description faust . App Instance of Faust __init__ ( * , broker , datadir = None , store = StoreEnum . AEROSPIKE . value , application_name = 'dagger' , package_name = 'dagger' , kafka_partitions = 1 , task_update_topic = None , tasks_topic = 'dagger_task_topic' , bootstrap_topic = 'dagger_bootstrap' , beacon = None , kafka_broker_list = None , loop = None , tracing_sensor = None , datadog_sensor = None , trigger_interval = 60 , max_tasks_per_trigger = 2000 , restart_tasks_on_boot = True , aerospike_config = None , enable_changelog = True , max_correletable_keys_in_values = 15000 , schema_registry_url = None , message_serializer = None , delete_workflow_on_complete = False , task_update_callbacks = [], ** kwargs ) Initialize an instance of Dagger. Parameters: Name Type Description Default broker str Kafka broker address i.e. kafka://0.0.0.0:9092. Defaults to None. required datadir str Directory where db data will reside. Defaults to None. None store str DB to use. Defaults to \"rocksdb://\". StoreEnum.AEROSPIKE.value application_name str Name of application. Defaults to \"dagger\". 'dagger' package_name str Name of package. Defaults to \"dagger\". 'dagger' kafka_partitions int Number of Kafka partitions. Defaults to 1. 1 task_update_topic Optional [ str ] Name of topic where tasks that have updated in status will be sent. Defaults to \"task_update_topic\". None tasks_topic str Name of topic where new tasks will be sent for execution. Defaults to \"dagger_task_topic\". 'dagger_task_topic' bootstrap_topic str Name of topic where tasks after restart will be sent for execution. Defaults to \"dagger_task_topic\". 'dagger_bootstrap' beacon NodeT Beacon used to track services in a dependency graph. Defaults to None. None loop asyncio . AbstractEventLoop Asyncio event loop to attach to. Defaults to None. None tracing_sensor Sensor Tracing Sensor to use for OpenTelemetry. The global tracer has to be initialized in the client. Defaults to None None datadog_sensor DatadogMonitor datadog statsD sensor None aerospike_config AerospikeConfig Config for Aerospike if enabled None enable_changelog bool Flag to enable/disable events on the table changelog topic True max_correletable_keys_in_values int maximum number of ids in the value part to chunk 15000 schema_registry_url str the schema registry URK None message_serializer MessageSerializer the message serializer instance using the schema registry None delete_workflow_on_complete bool deletes the workflow instance when complete False task_update_callbacks List [ Callable [[ ITemplateDAGInstance ], Awaitable [None]]] callbacks when a workflow instance is updated [] **kwargs Any Other Faust keyword arguments. {} __post_init__ () This method is called after initialization of dagger add_topic ( topic_name , topic ) Associate a topic instance with a name. Parameters: Name Type Description Default topic_name str Name of topic. required topic Topic Instance of Topic. required chunk_and_store_correlatable_tasks ( cor_instance , value , workflow_id ) async This method chunks the value of the key to the list so that we don't overflow the limit of the value size on the datastore used by dagger defined by max_correletable_keys_in_values Parameters: Name Type Description Default cor_instance CorreletableKeyTasks the CorreletableKeyTasks instance required value UUID the new value to add to the lookup_keys required workflow_id UUID the id of the workflow to which the cor_instance belongs to required create_topic ( topic_name , key_type = str , value_type = str ) classmethod Create a Kafka topic using Faust Parameters: Name Type Description Default topic_name str Name of topic required key_type type Key type for the topic. Defaults to str. str value_type type Value type for the topic. Defaults to str. str Returns: Type Description TopicT Instance of Faust Topic get_correletable_key_instances ( cor_instance ) async Gathers all the chunked values of the cor_instance Parameters: Name Type Description Default cor_instance CorreletableKeyTasks the CorreletableKeyTasks to gather required Returns: Type Description List [ CorreletableKeyTasks ] A list of all the chunked CorreletableKeyTasks get_db_options () Get the DB options on the dagger store Returns: Type Description Mapping [ str , Any ] the DB options get_instance ( id , log = True ) async Get an instance of an ITask given it's id. Parameters: Name Type Description Default id UUID Id of the ITask. required log bool suppress logging if set to True True Returns: Type Description ITask Instance of the ITask. get_monitoring_task ( task , workflow_instance ) async Returns the monitoring task associated with this task Parameters: Name Type Description Default task ITask the task to check for required Returns: Type Description Optional [ MonitoringTask ] the monitoring task instance or None get_template ( template_name ) Get the instance of a template that contains the workflow definition given it's name. Parameters: Name Type Description Default template_name str Name of template required Returns: Type Description ITemplateDAG Instance of Template that contains the workflow definition Raises: Type Description TemplateDoesNotExist Template does not exist get_topic ( topic_name ) Get a topic based on the associated name. Parameters: Name Type Description Default topic_name str Name of topic. required Returns: Type Description TopicT the instance of the topic main ( override_logging = False ) Main method that initializes the Dagger worker. This method is blocking. register_process_template ( process_template_name ) classmethod Registers a process template in Dagger. Parameters: Name Type Description Default (str) process_template_name Name of process to register in dagger. required register_template ( template_name ) classmethod this method is used to register a workflow definition with Dagger. Refer to the examples in the documentation Parameters: Name Type Description Default template_name str Name of workflow to register required remove_task_from_correletable_keys_table ( task , workflow_instance ) async Removes a task from the correletable keys table Parameters: Name Type Description Default task ITask the task to remove from the correletable keys table required workflow_instance ITemplateDAGInstance the workflow instance required submit ( task , * , repartition = True ) async Submits a workflow instance for execution. Parameters: Name Type Description Default task ITask The workflow instance. required repartition bool if True it uses the the repartitioning key to submit the task for execution on the configured kafka topic. If false, it creates the workflow on the same node and submits it for execution True update_correletable_key_for_task ( task_instance , key = None , new_task = False , workflow_instance = None ) async Updates the correletable key for a SensorTask within the datastore. Parameters: Name Type Description Default task_instance ITask the SensorTask for which the correletable key is updated required key str the correletable key name to update. None new_task bool If the task is not new, then the entire runtime paramters and subsequent tasks need to be updated False workflow_instance ITemplateDAGInstance the workflow instance None ITemplateDAGBuilder Base class to define the structure of workflow definition __init__ ( app ) Parameters: Name Type Description Default app Service The Dagger app instance required build () abstractmethod Builds the ITemplateDAGBuilder object. Returns: Type Description ITemplateDAG The instance of ITemplateDAG set_name ( name ) abstractmethod Sets the name of the template. Parameters: Name Type Description Default name str Name of the template. required Returns: Type Description ITemplateDAGBuilder An instance of the updated template builder. set_root ( template ) abstractmethod Sets the first process to execute in the template. Parameters: Name Type Description Default template IProcessTemplateDAG Instance of a process template containing the defintion of the process. required Returns: Type Description ITemplateDAGBuilder An instance of the updated template builder. set_type ( template_type ) abstractmethod Sets the type of the process. Parameters: Name Type Description Default template_type Type [ ITemplateDAGInstance ] The type of the process. required Returns: Type Description ITemplateDAGBuilder An instance of the updated template builder. IProcessTemplateDAGBuilder Skeleton builder class used to build a process definition within a workfow. __init__ ( app ) Parameters: Name Type Description Default app Service The Dagger instance required build () abstractmethod Builds the IProcessTemplateDAG object Returns: Type Description IProcessTemplateDAG the instance of IProcessTemplateDAG to create the workflow definition set_name ( process_name ) abstractmethod Sets the name of the process. Parameters: Name Type Description Default process_name str Name of the process. required Returns: Type Description IProcessTemplateDAGBuilder An instance of the updated process template builder. set_next_process ( task ) abstractmethod Set the next process in the execution of the workflow defintion. Parameters: Name Type Description Default task IProcessTemplateDAG TaskTemplate to be set as the next process. required Returns: Type Description IProcessTemplateDAGBuilder An instance of the updated process template builder. set_root_task ( task ) abstractmethod Set the first task in the process definition of the workflow. Parameters: Name Type Description Default task TaskTemplate TaskTemplate to be set as the first task of the process execution. required Returns: Type Description IProcessTemplateDAGBuilder An instance of the updated process template builder. set_type ( process_type ) abstractmethod Sets the type of the process. Parameters: Name Type Description Default process_type Type [ ITask ] The type of the process. required Returns: Type Description IProcessTemplateDAGBuilder An instance of the updated process template builder. TaskTemplateBuilder Skeleton builder class used to build the definition of a task object within a workflow __init__ ( app ) Parameters: Name Type Description Default app Service The Dagger object required build () abstractmethod Builds the TaskTemplate object Returns: Type Description TaskTemplate The TaskTemplate instance to add to the ProcessBuilder definition set_allow_skip_to ( allow_skip_to ) Set whether or not this task is allowed to be executed out of order (skipped to) Parameters: Name Type Description Default allow_skip_to bool If set to true a Sensor task can be executed if an event is received out of order required Returns: Type Description TaskTemplateBuilder An instance of the updated template builder. set_name ( name ) abstractmethod Set the name of task Parameters: Name Type Description Default name str the name of the Task required Returns: Type Description TaskTemplateBuilder An instance of the updated template builder. set_next ( task_template ) Set the next task in the process definition within the workflow Parameters: Name Type Description Default task_template TaskTemplate the next task template definition required Returns: Type Description TaskTemplateBuilder An instance of the updated template builder. set_type ( task_type ) abstractmethod Sets the type of task Parameters: Name Type Description Default task_type Type [ ITask ] the type of the task to instantiate from the builder required Returns: Type Description TaskTemplateBuilder An instance of the updated template builder. ITemplateDAG Skeleton class defining the attributes and functions of a template instance. __init__ ( dag , name , app , template_type ) ITemplateDAG Constructor Parameters: Name Type Description Default dag IProcessTemplateDAG The definition of the first process to execute required name str the name of the Workflow required app Service the Dagger instance required template_type Type [ ITemplateDAGInstance ] the type of the Workflow to instantiate required create_instance ( id , partition_key_lookup , * , repartition = True , seed = None , ** kwargs ) abstractmethod async Method for creating an instance of a workflow definition Parameters: Name Type Description Default id UUID The id of the workflow required partition_key_lookup str Kafka topic partition key associated with the instance of the workflow. The key needs to be defined within the runtime parameters required repartition bool Flag indicating if the creation of this instance needs to be stored on the current node or by the owner of the partition defined by the partition_key_lookup True seed random . Random the seed to use to create all internal instances of the workflow None **kwargs Other keyword arguments {} Returns: Type Description ITemplateDAGInstance An instance of the workflow set_dynamic_builders_for_process_template ( name , process_template_builders ) abstractmethod Use these builders only when the processes of the workflow definition need to be determined at runtime. Using these the processes in the workflow definition can be defined at runtime based on the runtime parameters of the workflow Parameters: Name Type Description Default name str Then name of the dynamic process builder required process_template_builders the ProcessTemplateDagBuilder dynamic process builders required IProcessTemplateDAG Skeleton class defining the attributes and functions of a process instance. __init__ ( next_process_dag , app , name , process_type , root_task_dag , max_run_duration ) Constructor Parameters: Name Type Description Default next_process_dag List [ IProcessTemplateDAG ] The list of next processes to execute in this workflow required app Service The Dagger instance required name str the name of the process required process_type Type [ ITask ] the type of the process to instantiate required root_task_dag Optional [ TaskTemplate ] The workflow type instance(definition) required max_run_duration int The maximum time this process can execute until its marked as failure(timeout) required create_instance ( id , parent_id , parent_name , partition_key_lookup , * , repartition = True , seed = None , workflow_instance = None , ** kwargs ) abstractmethod async Method for creating an instance of a process instance Parameters: Name Type Description Default id UUID The id of the instance required parent_id UUID the id of the parent of this process(The workflow instance) required parent_name str the name of the workflow required partition_key_lookup str The kafka partitioning key to look up required repartition bool If true the instance is serialized and stored in the owner of the parition owned by the partioning key True seed random . Random The seed to use to create any child instances None workflow_instance ITemplateDAGInstance the instance of the workflow None **kwargs Other keyword arguments {} Returns: Type Description IProcessTemplateDAGInstance the instance of the Process set_dynamic_process_builders ( process_template_builders ) abstractmethod Sets the dynamic process builders on the process instance determined at runtime. Only used when the processes of the workflow cannot be determined statically and needs to be defined at runtime Parameters: Name Type Description Default process_template_builders the list of ProcessTemplateDagBuilder required set_parallel_process_template_dags ( parallel_process_templates ) abstractmethod Method to set child_process_task_templates for parallel execution Parameters: Name Type Description Default parallel_process_templates List of parallel process templates required IDynamicProcessTemplateDAG Bases: IProcessTemplateDAG Skeleton class defining the attributes and functions of dynamic processes and task instances. __init__ ( next_process_dags , app , name , max_run_duration ) Init method Parameters: Name Type Description Default next_process_dags List [ IProcessTemplateDAG ] The next Process to execute after the current process is complete required app Service The Dagger instance required name str The name of the process required max_run_duration int the timeout on the process required create_instance ( id , parent_id , parent_name , partition_key_lookup , * , repartition = True , seed = None , workflow_instance = None , ** kwargs ) async Method for creating an dynamic instance(s) of a template and return the head of the list Parameters: Name Type Description Default id UUID The id of the instance to create required parent_id UUID the id of the parent instance required parent_name str the name of the parent workflow required partition_key_lookup str The kafka partitioning key to use serialize the storage of this instance required repartition bool If true, the instance is stored on the node owning the parition defined by the partioning key True seed random . Random the seed to use any create any child instances None workflow_instance ITemplateDAGInstance the instance of the workflow None **kwargs Other keyword arguments {} TaskTemplate Skeleton class defining the attributes and functions of process and task instances. create_instance ( id , parent_id , parent_name , partition_key_lookup , * , repartition = True , seed = None , workflow_instance = None , ** kwargs ) abstractmethod async Method for creating an instance of a template definition Parameters: Name Type Description Default id UUID the ID of the instance required parent_id UUID the ID of the parent instance if any required parent_name str the name of the parent instance required partition_key_lookup str The kafka paritioning key required repartition bool If true the instance is serialized and stored by the node owning the parition defined by the partitioning key True seed random . Random the seed to use to create any child instanes None workflow_instance ITemplateDAGInstance the workflow object None **kwargs Other keyword arguments {} DefaultTaskTemplate Bases: TaskTemplate Default implementation of task template. __init__ ( app , type , name , task_dag_template , allow_skip_to , reprocess_on_message = False ) Init method Parameters: Name Type Description Default app Service The Dagger Instance required type Type [ ITask ] The type of the task defined within the workflow required name str the name of the task required task_dag_template List [ TaskTemplate ] The next task to execute after this task required allow_skip_to bool Set to true if processing of the worklow can skip to this task required reprocess_on_message bool the task is executed when invoked irresepective of the state of the task False create_instance ( id , parent_id , parent_name , partition_key_lookup , * , repartition = True , seed = None , workflow_instance = None , ** kwargs ) async Creates an instance of the task defined by this template Parameters: Name Type Description Default id UUID the id of the instance to create required parent_id UUID the id of the parent of this task to be created required parent_name str the name of the task of the parent of the task to be created required partition_key_lookup str the kafka partioning key if this instance needs to be repartitioned required repartition bool If true, the instance is stored in the node owning the partition defined by the paritioning key True seed random . Random the seed to use to create any child instances None workflow_instance ITemplateDAGInstance the workflow object None **kwargs Any other keywork arguments {} DefaultKafkaTaskTemplate Bases: DefaultTaskTemplate Default implementation of KafkaCommandTask. __init__ ( app , type , name , topic , task_dag_templates , allow_skip_to , reprocess_on_message = False ) Init method Parameters: Name Type Description Default app Service Dagger instance required type Type [ KafkaCommandTask [ KT , VT ]] The type of KafkaCommandTask required name str the name of the task required topic Topic The topic to send the command on required task_dag_templates List [ TaskTemplate ] the next task to execute after this task required allow_skip_to bool If true the execution of the workflow can jump to this task required reprocess_on_message bool if true, the task is executed irrespective of the state of this task False create_instance ( id , parent_id , parent_name , partition_key_lookup , * , repartition = True , seed = None , workflow_instance = None , ** kwargs ) async Method for creating an instance of a kafkaTask Parameters: Name Type Description Default id UUID The id of the instance required parent_id UUID the id of the parent of this process(The workflow instance) required parent_name str the name of the workflow required partition_key_lookup str The kafka partitioning key to look up required repartition bool If true the instance is serialized and stored in the owner of the parition owned by the partioning key True seed random . Random The seed to use to create any child instances None workflow_instance ITemplateDAGInstance the instance of the workflow None **kwargs Any Other keyword arguments {} Returns: Type Description ITask the instance of the Process DefaultTriggerTaskTemplate Bases: DefaultTaskTemplate Default implementation of TriggerTask. __init__ ( app , type , name , time_to_execute_key , task_dag_templates , allow_skip_to ) Init method Parameters: Name Type Description Default app Service The dagger instance required type Type [ TriggerTask [ KT , VT ]] The type of TriggerTask required name str the name of the task required time_to_execute_key str the key lookup in runtime paramters to execute the trigger required allow_skip_to bool Flag For skipping serial execution required create_instance ( id , parent_id , parent_name , partition_key_lookup , * , repartition = True , seed = None , workflow_instance = None , ** kwargs ) async Method for creating an instance of a TriggerTask Parameters: Name Type Description Default id UUID The id of the instance required parent_id UUID the id of the parent of this process(The workflow instance) required parent_name str the name of the workflow required partition_key_lookup str The kafka partitioning key to look up required repartition bool If true the instance is serialized and stored in the owner of the parition owned by the partioning key True seed random . Random The seed to use to create any child instances None workflow_instance ITemplateDAGInstance the instance of the workflow None **kwargs Any Other keyword arguments {} Returns: Type Description ITask the instance of the Process DefaultIntervalTaskTemplate Bases: DefaultTaskTemplate Default implementation of IntervalTask. __init__ ( app , type , name , time_to_execute_key , time_to_force_complete_key , interval_execute_period_key , task_dag_templates , allow_skip_to ) Init method Parameters: Name Type Description Default app Service Dagger instance required type Type [ IntervalTask [ KT , VT ]] The type of IntervalTask required name str the name of the task required time_to_execute_key str the key to lookup in the runtime paramters to trigger the task required time_to_force_complete_key str the key to lookup to timeout the task required interval_execute_period_key str the frequency of execution from trigger time to timeout until the task succeeds required task_dag_templates List [ TaskTemplate ] the next task to execute required allow_skip_to bool Flag to skip serial execution required create_instance ( id , parent_id , parent_name , partition_key_lookup , * , repartition = True , seed = None , workflow_instance = None , ** kwargs ) async Method for creating an instance of IntervalTask Parameters: Name Type Description Default id UUID The id of the instance required parent_id UUID the id of the parent of this process(The workflow instance) required parent_name str the name of the workflow required partition_key_lookup str The kafka partitioning key to look up required repartition bool If true the instance is serialized and stored in the owner of the parition owned by the partioning key True seed random . Random The seed to use to create any child instances None workflow_instance ITemplateDAGInstance the instance of the workflow None **kwargs Other keyword arguments {} Returns: Type Description ITask the instance of the Process ParallelCompositeTaskTemplate Bases: DefaultTaskTemplate Template to define the structure of parallel tasks to execute within a workflow __init__ ( app , type , name , task_dag_templates , child_task_templates , allow_skip_to , reprocess_on_message = False , parallel_operator_type = TaskOperator . JOIN_ALL ) Init method Parameters: Name Type Description Default app Service The dagger instance required type Type [ ITask ] The type of ParallelCompositeTask required name str The name of the ParallelCompositeTask required task_dag_templates List [ TaskTemplate ] the next task to execute after the parallel task completes required child_task_templates List [ TaskTemplate ] the set of parallel tasks to execute required allow_skip_to bool Skip serial execution of the workflow if this is set required reprocess_on_message bool Re-execute the task irrespective of the state of the task False parallel_operator_type TaskOperator Wait for all parallel tasks to complete or just one before transitioning to the next task in the workflow TaskOperator.JOIN_ALL DefaultTaskTemplateBuilder Bases: TaskTemplateBuilder Default Implementation of TaskTemplateBuilder ParallelCompositeTaskTemplateBuilder Bases: DefaultTaskTemplateBuilder A type of DefaultTaskTemplateBuilder to create ParallelTasks KafkaCommandTaskTemplateBuilder Bases: DefaultTaskTemplateBuilder A type of DefaultTaskTemplateBuilder to define KafkaCommandTasks DecisionTaskTemplateBuilder Bases: DefaultTaskTemplateBuilder A type of DefaultTaskTemplateBuilder to define DecisionTasks TriggerTaskTemplateBuilder Bases: DefaultTaskTemplateBuilder A type of DefaultTaskTemplateBuilder to define TriggerTasks IntervalTaskTemplateBuilder Bases: TriggerTaskTemplateBuilder A type of DefaultTaskTemplateBuilder to define IntervalTasks KafkaListenerTaskTemplateBuilder Bases: DefaultTaskTemplateBuilder A type of DefaultTaskTemplateBuilder to define KafkaListenerTasks set_reprocess_on_message ( reprocess_on_message ) Set whether or not this task should reprocess on_message if a correlated message is reprocessed. DefaultTemplateBuilder Bases: ITemplateDAGBuilder Default implementation of template builder build () Builds the DefaultTemplateBuilder Returns: Type Description ITemplateDAG the instance of ITemplateDAG from the builder definition set_name ( name ) Sets the name of the task Parameters: Name Type Description Default name str The name of the task the builder is setting up required Returns: Type Description ITemplateDAGBuilder The updated ITemplateDAGBuilder set_root ( template ) Sets the root process of this task Parameters: Name Type Description Default template IProcessTemplateDAG the parent Process Task required Returns: Type Description ITemplateDAGBuilder the instance of ITemplateDAGBuilder set_type ( template_type ) Sets the type of the Task to instantiate from this builder Parameters: Name Type Description Default template_type Type [ ITemplateDAGInstance ] the Template Type required Returns: Type Description ITemplateDAGBuilder the instance of ITemplateDAGBuilder ProcessTemplateDAG Bases: IProcessTemplateDAG Class that encapsulates the definition of a Process __init__ ( next_process_dag , app , name , process_type , root_task_dag , max_run_duration ) Constructor Parameters: Name Type Description Default next_process_dag List [ IProcessTemplateDAG ] The list of next processes to execute in this workflow required app Dagger The Dagger instance required name str the name of the process required process_type Type [ ITask [ KT , VT ]] the type of the process to instantiate required root_task_dag Optional [ TaskTemplate ] The workflow type instance(definition) required max_run_duration int The maximum time this process can execute until its marked as failure(timeout) required create_instance ( id , parent_id , parent_name , partition_key_lookup , * , repartition = True , seed = None , workflow_instance = None , ** kwargs ) async Method for creating an instance of a process instance Parameters: Name Type Description Default id UUID The id of the instance required parent_id UUID the id of the parent of this process(The workflow instance) required parent_name str the name of the workflow required partition_key_lookup str The kafka partitioning key to look up required repartition bool If true the instance is serialized and stored in the owner of the parition owned by the partioning key True seed random . Random The seed to use to create any child instances None workflow_instance ITemplateDAGInstance the instance of the workflow None **kwargs Any Other keyword arguments {} Returns: Type Description IProcessTemplateDAGInstance [ KT , VT ] the instance of the Process ParallelCompositeProcessTemplateDAG Bases: ProcessTemplateDAG A Process Template to define a set of parallel Processes within a Process __init__ ( next_process_dag , app , name , process_type , child_process_task_templates , parallel_operator_type ) init method Parameters: Name Type Description Default next_process_dag List [ IProcessTemplateDAG ] The next Process in the workflow definition required app Dagger The Dagger instance required name str The name of the ParallelCompositeProcessTask required process_type Type [ ParallelCompositeTask [ KT , VT ]] The type of ParallelCompositeTask to be instantiated from the definition required child_process_task_templates List [ IProcessTemplateDAG ] The list of parallel processes to be created required parallel_operator_type TaskOperator Wait for either all to complete or just one before transitioning to the next process in the workflow required create_instance ( id , parent_id , parent_name , partition_key_lookup , * , repartition = True , seed = None , workflow_instance = None , ** kwargs ) async Create a ParallelCompositeTask instance based on the template definition Parameters: Name Type Description Default id UUID The id of the instance required parent_id UUID the id of the parent of this process(The workflow instance) required parent_name str the name of the workflow required partition_key_lookup str The kafka partitioning key to look up required repartition bool If true the instance is serialized and stored in the owner of the parition owned by the partioning key True seed random . Random The seed to use to create any child instances None workflow_instance ITemplateDAGInstance the instance of the workflow None **kwargs Any Other keyword arguments {} Returns: Type Description ParallelCompositeTask [ KT , VT ] the instance of the Process set_parallel_process_template_dags ( parallel_process_templates ) Sets child_process_task_templates Parameters: Name Type Description Default parallel_process_templates List [ IProcessTemplateDAG ] List of parallel process templates required TemplateDAG Bases: ITemplateDAG Default Implementation of ITemplateDAG __init__ ( dag , name , app , template_type ) Constructor Parameters: Name Type Description Default dag IProcessTemplateDAG The definition of the first process to execute required name str the name of the Workflow required app Dagger the Dagger instance required template_type Type [ ITemplateDAGInstance [ KT , VT ]] the type of the Workflow to instantiate required create_instance ( id , partition_key_lookup , * , repartition = True , seed = None , ** kwargs ) async Method for creating an instance of a workflow definition Parameters: Name Type Description Default id UUID The id of the workflow required partition_key_lookup str Kafka topic partition key associated with the instance of the workflow. The key needs to be defined within the runtime parameters required repartition bool Flag indicating if the creation of this instance needs to be stored on the current node or by the owner of the partition defined by the partition_key_lookup True seed random . Random the seed to use to create all internal instances of the workflow None **kwargs Other keyword arguments {} Returns: Type Description ITemplateDAGInstance [ KT , VT ] An instance of the workflow get_given_process ( process_name ) Looks up for a specific process template within a DAG Parameters: Name Type Description Default process_name str Name of the process required Returns: Type Description Optional [ IProcessTemplateDAG ] Process Template if found, else None set_given_num_of_parallel_processes_for_a_composite_process ( no_of_processes , composite_process_name , parallel_template_builder ) This method creates and sets 'N' number of new parralel processes for a given process in a DAG Parameters: Name Type Description Default no_of_processes int Number of parallel processes required required composite_process_name str Name of the process with in which the parallel processes must reside required parallel_template_builder ProcessTemplateDagBuilder A process template builder which needs to be cloned 'N' times and executed in parallel required set_parallel_process_template_dags_for_a_composite_process ( name , parallel_process_templates ) Sets new process builders within a given process in a DAG Parameters: Name Type Description Default name str Name of the process with in which given process builders must reside required parallel_process_templates List [ IProcessTemplateDAG ] List of process template builders required DynamicProcessTemplateDAG Bases: IDynamicProcessTemplateDAG A template to add Dynamic Processes to a workflow at runtime __init__ ( next_process_dag , app , name , max_run_duration ) Init method Parameters: Name Type Description Default next_process_dag List [ IProcessTemplateDAG ] the next process in the workflow definition required app Dagger The Dagger instance required name str the name of the Dynamic Process required max_run_duration int The timeout on the process to COMPLETE execution required create_instance ( id , parent_id , parent_name , partition_key_lookup , * , repartition = True , seed = None , workflow_instance = None , ** kwargs ) async Create a ParallelCompositeTask instance based on the template definition Parameters: Name Type Description Default id UUID The id of the instance required parent_id UUID the id of the parent of this process(The workflow instance) required parent_name str the name of the workflow required partition_key_lookup str The kafka partitioning key to look up required repartition bool If true the instance is serialized and stored in the owner of the parition owned by the partioning key True seed random . Random The seed to use to create any child instances None workflow_instance ITemplateDAGInstance the instance of the workflow None **kwargs Any Other keyword arguments {} Returns: Type Description IProcessTemplateDAGInstance [ KT , VT ] the instance of the Process ProcessTemplateDagBuilder Bases: IProcessTemplateDAGBuilder Default implementation of process builder __init__ ( app ) Init method Parameters: Name Type Description Default app Dagger The Dagger instance required DynamicProcessTemplateDagBuilder Bases: IProcessTemplateDAGBuilder Skeleton builder class used to build a dynamic process object(s). set_root_task ( task ) Not implemented. Raises: Type Description NotImplementedError Not implemented. set_type ( process_type ) Not implemented. Raises: Type Description NotImplementedError Not implemented. ParallelCompositeProcessTemplateDagBuilder Bases: IProcessTemplateDAGBuilder Skeleton builder class used to build a parallel process object(s). set_max_run_duration ( max_run_duration ) Not implemented. Raises: Type Description NotImplementedError Not implemented. set_root_task ( task ) Not implemented. Raises: Type Description NotImplementedError Not implemented. TaskStatusEnum Bases: Enum Class to indicate State of the Task COMPLETED = 'COMPLETED' class-attribute The Task COMPLETED Execution EXECUTING = 'EXECUTING' class-attribute The task is currently EXECUTING FAILURE = 'FAILURE' class-attribute The Task Failed during Execution NOT_STARTED = 'NOT_STARTED' class-attribute The Task has NOT STARTED EXECUTION SKIPPED = 'SKIPPED' class-attribute The Task Skipped Execution STOPPED = 'STOPPED' class-attribute The Task execution was STOPPED SUBMITTED = 'SUBMITTED' class-attribute The Task was SUBMITTED for Execution TaskType Bases: Enum The type of the Task LEAF = 'LEAF' class-attribute Task which has no children PARALLEL_COMPOSITE = 'PARALLEL_COMPOSITE' class-attribute A container for parallel tasks ROOT = 'ROOT' class-attribute The Root node of the workflow SUB_DAG = 'SUB_DAG' class-attribute A Process Task that consists of LEAF Tasks TaskStatus Bases: Record Class to serialize the status of the Task ITask Bases: Record , Generic [ KT , VT ] Class that every template, process, and task extends. Defines attributes and core functions that Dagger uses. evaluate ( ** kwargs ) abstractmethod async Processes some inputs and determines the next ITask id. Returns: Type Description Optional [ UUID ] The next ITask id. execute ( runtime_parameters , workflow_instance = None ) abstractmethod async Executes the ITask. Parameters: Name Type Description Default workflow_instance ITask The workflow object None get_correlatable_key ( payload ) abstractmethod Get the lookup key,value associated with the task.Deprecated use get_correlatable_key_from_payload Parameters: Name Type Description Default payload Any The lookup key,value. required Returns: Type Description TaskLookupKey used to associate a task with a message. get_correlatable_key_from_payload ( payload ) async Get the lookup key,value associated with the task(Deprecated use get_correlatable_keys_from_payload). Parameters: Name Type Description Default payload Any The lookup key,value. required Returns: Type Description TaskLookupKey used to associate a task with a message. get_correlatable_keys_from_payload ( payload ) async Get a list of lookup key,value associated with the task(s). Parameters: Name Type Description Default payload Any The lookup key,value. required Returns: Type Description List [ TaskLookupKey ] used to associate a tasks with a message. get_remaining_tasks ( next_dag_id , workflow_instance , tasks = None , end_task_id = None ) async Get the remaining tasks in the workflow. Parameters: Name Type Description Default next_dag_id UUID Current ITask id. required tasks Optional [ List [ ITask ]] List of previous ITasks. Defaults to []. None end_task_id UUID The task id that the function should stop and return at. Defaults to None (so end of DAG). None workflow_instance ITemplateDAGInstance The Workflow object required Returns: Type Description Optional [ List [ ITask ]] List of remaining ITasks appended to inputted list. notify ( status , workflow_instance ) async If not completed, runs the steps required for completion by calling on_complete(). This is used to signal a task that it can now complete Parameters: Name Type Description Default status TaskStatus the status of the task to set to when completed required workflow_instance Optional [ ITemplateDAGInstance ] the Workflow object required on_complete ( workflow_instance , status = TaskStatus ( code = TaskStatusEnum . COMPLETED . name , value = TaskStatusEnum . COMPLETED . value ), * , iterate = True ) async Sets the status of the ITask to completed and starts the next ITask if there is one. Parameters: Name Type Description Default workflow_instance Optional [ ITemplateDAGInstance ] The workflow object required status TaskStatus The status of the task to set to TaskStatus(code=TaskStatusEnum.COMPLETED.name, value=TaskStatusEnum.COMPLETED.value) on_message ( runtime_parameters , * args , ** kwargs ) abstractmethod async Defines what to do when the task recieves a message. Parameters: Name Type Description Default runtime_parameters Dict [ str , str ] The runtime parameters of the task required Returns: Type Description bool True if the processing succeeds false otherwise start ( workflow_instance ) abstractmethod async Starts the ITask. Parameters: Name Type Description Default workflow_instance Optional [ ITemplateDAGInstance ] The Workflow instance required stop () abstractmethod async Stops the ITask. MonitoringTask Bases: TriggerTask [ KT , VT ] , abc . ABC A Type of TriggerTask that executes at s specific time and checks on the monitored task to execute some domain specific logic process_monitored_task ( monitored_task , workflow_instance ) abstractmethod async Callback on when business logic has to be executed on the monitored task based on the time condition Parameters: Name Type Description Default monitored_task ITask the monitored task required workflow_instance Optional [ ITemplateDAGInstance ] the workflow object required Returns: Type Description None None IntervalTask Bases: TriggerTask [ KT , VT ] , abc . ABC A type of Task to Trigger at a trigger time and execute multiple times until the execution completes. The task is retried until the timeout is reached periodically after the trigger time interval_execute ( runtime_parameters ) async Task to run on an interval until either the trigger end time or until this method returns True. Parameters: Name Type Description Default runtime_parameters Dict [ str , VT ] The runtime parameters of the task required Returns: Type Description bool If True, finish this task. TriggerTask Bases: ExecutorTask [ KT , VT ] , abc . ABC This task waits/halts the execution of the DAG until current time >= the trigger time on the task and then invokes the execute method defined by the task ExecutorTask Bases: ITask [ KT , VT ] , abc . ABC A simple ITask that executes some domain specific logic evaluate ( ** kwargs ) async Not implemented. Raises: Type Description NotImplementedError Not implemented. on_message ( runtime_parameters , * args , ** kwargs ) async Not implemented. Raises: Type Description NotImplementedError Not implemented. DefaultMonitoringTask Bases: MonitoringTask [ str , str ] Default Implementation of MonitoringTask DecisionTask Bases: ITask [ KT , VT ] This type of task is similar to the case..switch statement in a programming language. It returns the next task to execute based on the execution logic. A decision task needs to implement execute ( runtime_parameters , workflow_instance = None ) async Not implemented. Raises: NotImplementedError: Not implemented. on_message ( runtime_parameters , * args , ** kwargs ) async Not implemented. Raises: NotImplementedError: Not implemented. SystemTask Bases: ExecutorTask [ str , str ] An internal Task for Dagger bookkeeping evaluate ( ** kwargs ) async Not implemented. Raises: Type Description NotImplementedError Not implemented. get_correlatable_key ( payload ) Not implemented. Raises: Type Description NotImplementedError Not implemented. on_complete ( workflow_instance , status = TaskStatus ( code = TaskStatusEnum . COMPLETED . name , value = TaskStatusEnum . COMPLETED . value ), iterate = True ) async Not implemented. Raises: Type Description NotImplementedError Not implemented. on_message ( * args , ** kwargs ) async Not implemented. Raises: Type Description NotImplementedError Not implemented. SystemTimerTask Bases: SystemTask A type of SystemTask to execute internal Dagger Tasks SensorTask Bases: ITask [ KT , VT ] , abc . ABC A type of task that halts execution of the workflow until a condition is met. When the condition is met the on_message method on this task is invoked evaluate ( ** kwargs ) async Not implemented. Raises: Type Description NotImplementedError Not implemented. execute ( runtime_parameters , workflow_instance = None ) async Not implemented. Raises: Type Description NotImplementedError Not implemented. IMonitoredTask Abstract interface to enable monitoring of a task get_monitoring_task_type () abstractmethod Get the TaskType to instantiate to monitor the current task Returns: Type Description Type [ MonitoringTask ] The Type of MonitoringTask KafkaCommandTask Bases: ExecutorTask [ KT , VT ] , abc . ABC This task is used to send a request/message on a Kafka Topic defined using the template builder. This type of task is a child task in the execution graph and can be extended by implementing the method KafkaListenerTask Bases: SensorTask [ KT , VT ] , abc . ABC This task waits/halts the execution of the DAG until a message is received on the defined Kafka topic(in the template definition). Each task is created using the DAG builder defines a durable key to correlate each received message on the topic against listener tasks. The Engine handles the complexity of invoking the appropriate task instance based on the key in the payload. INonLeafNodeTask Bases: ITask [ KT , VT ] , abc . ABC An Abstract class for any Process/SUB_DAG node TaskOperator Bases: Enum An operator for Joining Parallel Tasks ATLEAST_ONE = 'ATLEAST_ONE' class-attribute Wait for Atleast one of the parallel tasks to reach terminal state to begin execution of the next task in the workflow definition JOIN_ALL = 'JOIN_ALL' class-attribute Waits for All the parallel tasks to reach terminal state before execution of the next task in the workflow definition ParallelCompositeTask Bases: ITask [ KT , VT ] , abc . ABC SUB-DAG Task to execute parallel tasks and wait until all of them are in a terminal state before progressing to the next task This task can be embedded as a child of the root node or a process node notify ( status , workflow_instance = None ) async If not completed, runs the steps required for completion by calling on_complete(). IProcessTemplateDAGInstance Bases: INonLeafNodeTask [ KT , VT ] , abc . ABC A Process implementation of INonLeafNodeTask CorrelatableMapValue Bases: Record An internal Class to store the correletable keys and their associated values for SensorTask ITemplateDAGInstance Bases: INonLeafNodeTask [ KT , VT ] , abc . ABC A root node implementation of INonLeafNodeTask MonitoredProcessTemplateDAGInstance Bases: DefaultProcessTemplateDAGInstance , IMonitoredTask Default implementation of a Monitored ProcessTask on_complete ( workflow_instance , status = TaskStatus ( code = TaskStatusEnum . COMPLETED . name , value = TaskStatusEnum . COMPLETED . value ), iterate = True ) async Sets the status of the ITask to completed and starts the next ITask if there is one. Trigger Bases: Record Class To store the Trigger data. The time to execute a task get_trigger_key () The key to store for the trigger instance Returns: Type Description Tuple [ Optional [ UUID ], Optional [ UUID ]] The Key to store DaggerError Bases: Exception Base-class for all Dagger exceptions. InvalidTaskType Bases: DaggerError Invalid Task Type. TaskInvalidState Bases: DaggerError Invalid Task State. TemplateDoesNotExist Bases: DaggerError Invalid Template Name InvalidTriggerTimeForTask Bases: DaggerError Invalid trigger time","title":"API Documentation"},{"location":"api/#api-documentation","text":"","title":"API Documentation"},{"location":"api/#dagger.service.services.Dagger","text":"Bases: Service The Dagger class to create the workflow engine.","title":"Dagger"},{"location":"api/#dagger.service.services.Dagger.__create_app","text":"Initializes instance of Faust Returns: Type Description faust . App Instance of Faust","title":"__create_app()"},{"location":"api/#dagger.service.services.Dagger.__init__","text":"Initialize an instance of Dagger. Parameters: Name Type Description Default broker str Kafka broker address i.e. kafka://0.0.0.0:9092. Defaults to None. required datadir str Directory where db data will reside. Defaults to None. None store str DB to use. Defaults to \"rocksdb://\". StoreEnum.AEROSPIKE.value application_name str Name of application. Defaults to \"dagger\". 'dagger' package_name str Name of package. Defaults to \"dagger\". 'dagger' kafka_partitions int Number of Kafka partitions. Defaults to 1. 1 task_update_topic Optional [ str ] Name of topic where tasks that have updated in status will be sent. Defaults to \"task_update_topic\". None tasks_topic str Name of topic where new tasks will be sent for execution. Defaults to \"dagger_task_topic\". 'dagger_task_topic' bootstrap_topic str Name of topic where tasks after restart will be sent for execution. Defaults to \"dagger_task_topic\". 'dagger_bootstrap' beacon NodeT Beacon used to track services in a dependency graph. Defaults to None. None loop asyncio . AbstractEventLoop Asyncio event loop to attach to. Defaults to None. None tracing_sensor Sensor Tracing Sensor to use for OpenTelemetry. The global tracer has to be initialized in the client. Defaults to None None datadog_sensor DatadogMonitor datadog statsD sensor None aerospike_config AerospikeConfig Config for Aerospike if enabled None enable_changelog bool Flag to enable/disable events on the table changelog topic True max_correletable_keys_in_values int maximum number of ids in the value part to chunk 15000 schema_registry_url str the schema registry URK None message_serializer MessageSerializer the message serializer instance using the schema registry None delete_workflow_on_complete bool deletes the workflow instance when complete False task_update_callbacks List [ Callable [[ ITemplateDAGInstance ], Awaitable [None]]] callbacks when a workflow instance is updated [] **kwargs Any Other Faust keyword arguments. {}","title":"__init__()"},{"location":"api/#dagger.service.services.Dagger.__post_init__","text":"This method is called after initialization of dagger","title":"__post_init__()"},{"location":"api/#dagger.service.services.Dagger.add_topic","text":"Associate a topic instance with a name. Parameters: Name Type Description Default topic_name str Name of topic. required topic Topic Instance of Topic. required","title":"add_topic()"},{"location":"api/#dagger.service.services.Dagger.chunk_and_store_correlatable_tasks","text":"This method chunks the value of the key to the list so that we don't overflow the limit of the value size on the datastore used by dagger defined by max_correletable_keys_in_values Parameters: Name Type Description Default cor_instance CorreletableKeyTasks the CorreletableKeyTasks instance required value UUID the new value to add to the lookup_keys required workflow_id UUID the id of the workflow to which the cor_instance belongs to required","title":"chunk_and_store_correlatable_tasks()"},{"location":"api/#dagger.service.services.Dagger.create_topic","text":"Create a Kafka topic using Faust Parameters: Name Type Description Default topic_name str Name of topic required key_type type Key type for the topic. Defaults to str. str value_type type Value type for the topic. Defaults to str. str Returns: Type Description TopicT Instance of Faust Topic","title":"create_topic()"},{"location":"api/#dagger.service.services.Dagger.get_correletable_key_instances","text":"Gathers all the chunked values of the cor_instance Parameters: Name Type Description Default cor_instance CorreletableKeyTasks the CorreletableKeyTasks to gather required Returns: Type Description List [ CorreletableKeyTasks ] A list of all the chunked CorreletableKeyTasks","title":"get_correletable_key_instances()"},{"location":"api/#dagger.service.services.Dagger.get_db_options","text":"Get the DB options on the dagger store Returns: Type Description Mapping [ str , Any ] the DB options","title":"get_db_options()"},{"location":"api/#dagger.service.services.Dagger.get_instance","text":"Get an instance of an ITask given it's id. Parameters: Name Type Description Default id UUID Id of the ITask. required log bool suppress logging if set to True True Returns: Type Description ITask Instance of the ITask.","title":"get_instance()"},{"location":"api/#dagger.service.services.Dagger.get_monitoring_task","text":"Returns the monitoring task associated with this task Parameters: Name Type Description Default task ITask the task to check for required Returns: Type Description Optional [ MonitoringTask ] the monitoring task instance or None","title":"get_monitoring_task()"},{"location":"api/#dagger.service.services.Dagger.get_template","text":"Get the instance of a template that contains the workflow definition given it's name. Parameters: Name Type Description Default template_name str Name of template required Returns: Type Description ITemplateDAG Instance of Template that contains the workflow definition Raises: Type Description TemplateDoesNotExist Template does not exist","title":"get_template()"},{"location":"api/#dagger.service.services.Dagger.get_topic","text":"Get a topic based on the associated name. Parameters: Name Type Description Default topic_name str Name of topic. required Returns: Type Description TopicT the instance of the topic","title":"get_topic()"},{"location":"api/#dagger.service.services.Dagger.main","text":"Main method that initializes the Dagger worker. This method is blocking.","title":"main()"},{"location":"api/#dagger.service.services.Dagger.register_process_template","text":"Registers a process template in Dagger. Parameters: Name Type Description Default (str) process_template_name Name of process to register in dagger. required","title":"register_process_template()"},{"location":"api/#dagger.service.services.Dagger.register_template","text":"this method is used to register a workflow definition with Dagger. Refer to the examples in the documentation Parameters: Name Type Description Default template_name str Name of workflow to register required","title":"register_template()"},{"location":"api/#dagger.service.services.Dagger.remove_task_from_correletable_keys_table","text":"Removes a task from the correletable keys table Parameters: Name Type Description Default task ITask the task to remove from the correletable keys table required workflow_instance ITemplateDAGInstance the workflow instance required","title":"remove_task_from_correletable_keys_table()"},{"location":"api/#dagger.service.services.Dagger.submit","text":"Submits a workflow instance for execution. Parameters: Name Type Description Default task ITask The workflow instance. required repartition bool if True it uses the the repartitioning key to submit the task for execution on the configured kafka topic. If false, it creates the workflow on the same node and submits it for execution True","title":"submit()"},{"location":"api/#dagger.service.services.Dagger.update_correletable_key_for_task","text":"Updates the correletable key for a SensorTask within the datastore. Parameters: Name Type Description Default task_instance ITask the SensorTask for which the correletable key is updated required key str the correletable key name to update. None new_task bool If the task is not new, then the entire runtime paramters and subsequent tasks need to be updated False workflow_instance ITemplateDAGInstance the workflow instance None","title":"update_correletable_key_for_task()"},{"location":"api/#dagger.templates.template.ITemplateDAGBuilder","text":"Base class to define the structure of workflow definition","title":"ITemplateDAGBuilder"},{"location":"api/#dagger.templates.template.ITemplateDAGBuilder.__init__","text":"Parameters: Name Type Description Default app Service The Dagger app instance required","title":"__init__()"},{"location":"api/#dagger.templates.template.ITemplateDAGBuilder.build","text":"Builds the ITemplateDAGBuilder object. Returns: Type Description ITemplateDAG The instance of ITemplateDAG","title":"build()"},{"location":"api/#dagger.templates.template.ITemplateDAGBuilder.set_name","text":"Sets the name of the template. Parameters: Name Type Description Default name str Name of the template. required Returns: Type Description ITemplateDAGBuilder An instance of the updated template builder.","title":"set_name()"},{"location":"api/#dagger.templates.template.ITemplateDAGBuilder.set_root","text":"Sets the first process to execute in the template. Parameters: Name Type Description Default template IProcessTemplateDAG Instance of a process template containing the defintion of the process. required Returns: Type Description ITemplateDAGBuilder An instance of the updated template builder.","title":"set_root()"},{"location":"api/#dagger.templates.template.ITemplateDAGBuilder.set_type","text":"Sets the type of the process. Parameters: Name Type Description Default template_type Type [ ITemplateDAGInstance ] The type of the process. required Returns: Type Description ITemplateDAGBuilder An instance of the updated template builder.","title":"set_type()"},{"location":"api/#dagger.templates.template.IProcessTemplateDAGBuilder","text":"Skeleton builder class used to build a process definition within a workfow.","title":"IProcessTemplateDAGBuilder"},{"location":"api/#dagger.templates.template.IProcessTemplateDAGBuilder.__init__","text":"Parameters: Name Type Description Default app Service The Dagger instance required","title":"__init__()"},{"location":"api/#dagger.templates.template.IProcessTemplateDAGBuilder.build","text":"Builds the IProcessTemplateDAG object Returns: Type Description IProcessTemplateDAG the instance of IProcessTemplateDAG to create the workflow definition","title":"build()"},{"location":"api/#dagger.templates.template.IProcessTemplateDAGBuilder.set_name","text":"Sets the name of the process. Parameters: Name Type Description Default process_name str Name of the process. required Returns: Type Description IProcessTemplateDAGBuilder An instance of the updated process template builder.","title":"set_name()"},{"location":"api/#dagger.templates.template.IProcessTemplateDAGBuilder.set_next_process","text":"Set the next process in the execution of the workflow defintion. Parameters: Name Type Description Default task IProcessTemplateDAG TaskTemplate to be set as the next process. required Returns: Type Description IProcessTemplateDAGBuilder An instance of the updated process template builder.","title":"set_next_process()"},{"location":"api/#dagger.templates.template.IProcessTemplateDAGBuilder.set_root_task","text":"Set the first task in the process definition of the workflow. Parameters: Name Type Description Default task TaskTemplate TaskTemplate to be set as the first task of the process execution. required Returns: Type Description IProcessTemplateDAGBuilder An instance of the updated process template builder.","title":"set_root_task()"},{"location":"api/#dagger.templates.template.IProcessTemplateDAGBuilder.set_type","text":"Sets the type of the process. Parameters: Name Type Description Default process_type Type [ ITask ] The type of the process. required Returns: Type Description IProcessTemplateDAGBuilder An instance of the updated process template builder.","title":"set_type()"},{"location":"api/#dagger.templates.template.TaskTemplateBuilder","text":"Skeleton builder class used to build the definition of a task object within a workflow","title":"TaskTemplateBuilder"},{"location":"api/#dagger.templates.template.TaskTemplateBuilder.__init__","text":"Parameters: Name Type Description Default app Service The Dagger object required","title":"__init__()"},{"location":"api/#dagger.templates.template.TaskTemplateBuilder.build","text":"Builds the TaskTemplate object Returns: Type Description TaskTemplate The TaskTemplate instance to add to the ProcessBuilder definition","title":"build()"},{"location":"api/#dagger.templates.template.TaskTemplateBuilder.set_allow_skip_to","text":"Set whether or not this task is allowed to be executed out of order (skipped to) Parameters: Name Type Description Default allow_skip_to bool If set to true a Sensor task can be executed if an event is received out of order required Returns: Type Description TaskTemplateBuilder An instance of the updated template builder.","title":"set_allow_skip_to()"},{"location":"api/#dagger.templates.template.TaskTemplateBuilder.set_name","text":"Set the name of task Parameters: Name Type Description Default name str the name of the Task required Returns: Type Description TaskTemplateBuilder An instance of the updated template builder.","title":"set_name()"},{"location":"api/#dagger.templates.template.TaskTemplateBuilder.set_next","text":"Set the next task in the process definition within the workflow Parameters: Name Type Description Default task_template TaskTemplate the next task template definition required Returns: Type Description TaskTemplateBuilder An instance of the updated template builder.","title":"set_next()"},{"location":"api/#dagger.templates.template.TaskTemplateBuilder.set_type","text":"Sets the type of task Parameters: Name Type Description Default task_type Type [ ITask ] the type of the task to instantiate from the builder required Returns: Type Description TaskTemplateBuilder An instance of the updated template builder.","title":"set_type()"},{"location":"api/#dagger.templates.template.ITemplateDAG","text":"Skeleton class defining the attributes and functions of a template instance.","title":"ITemplateDAG"},{"location":"api/#dagger.templates.template.ITemplateDAG.__init__","text":"ITemplateDAG Constructor Parameters: Name Type Description Default dag IProcessTemplateDAG The definition of the first process to execute required name str the name of the Workflow required app Service the Dagger instance required template_type Type [ ITemplateDAGInstance ] the type of the Workflow to instantiate required","title":"__init__()"},{"location":"api/#dagger.templates.template.ITemplateDAG.create_instance","text":"Method for creating an instance of a workflow definition Parameters: Name Type Description Default id UUID The id of the workflow required partition_key_lookup str Kafka topic partition key associated with the instance of the workflow. The key needs to be defined within the runtime parameters required repartition bool Flag indicating if the creation of this instance needs to be stored on the current node or by the owner of the partition defined by the partition_key_lookup True seed random . Random the seed to use to create all internal instances of the workflow None **kwargs Other keyword arguments {} Returns: Type Description ITemplateDAGInstance An instance of the workflow","title":"create_instance()"},{"location":"api/#dagger.templates.template.ITemplateDAG.set_dynamic_builders_for_process_template","text":"Use these builders only when the processes of the workflow definition need to be determined at runtime. Using these the processes in the workflow definition can be defined at runtime based on the runtime parameters of the workflow Parameters: Name Type Description Default name str Then name of the dynamic process builder required process_template_builders the ProcessTemplateDagBuilder dynamic process builders required","title":"set_dynamic_builders_for_process_template()"},{"location":"api/#dagger.templates.template.IProcessTemplateDAG","text":"Skeleton class defining the attributes and functions of a process instance.","title":"IProcessTemplateDAG"},{"location":"api/#dagger.templates.template.IProcessTemplateDAG.__init__","text":"Constructor Parameters: Name Type Description Default next_process_dag List [ IProcessTemplateDAG ] The list of next processes to execute in this workflow required app Service The Dagger instance required name str the name of the process required process_type Type [ ITask ] the type of the process to instantiate required root_task_dag Optional [ TaskTemplate ] The workflow type instance(definition) required max_run_duration int The maximum time this process can execute until its marked as failure(timeout) required","title":"__init__()"},{"location":"api/#dagger.templates.template.IProcessTemplateDAG.create_instance","text":"Method for creating an instance of a process instance Parameters: Name Type Description Default id UUID The id of the instance required parent_id UUID the id of the parent of this process(The workflow instance) required parent_name str the name of the workflow required partition_key_lookup str The kafka partitioning key to look up required repartition bool If true the instance is serialized and stored in the owner of the parition owned by the partioning key True seed random . Random The seed to use to create any child instances None workflow_instance ITemplateDAGInstance the instance of the workflow None **kwargs Other keyword arguments {} Returns: Type Description IProcessTemplateDAGInstance the instance of the Process","title":"create_instance()"},{"location":"api/#dagger.templates.template.IProcessTemplateDAG.set_dynamic_process_builders","text":"Sets the dynamic process builders on the process instance determined at runtime. Only used when the processes of the workflow cannot be determined statically and needs to be defined at runtime Parameters: Name Type Description Default process_template_builders the list of ProcessTemplateDagBuilder required","title":"set_dynamic_process_builders()"},{"location":"api/#dagger.templates.template.IProcessTemplateDAG.set_parallel_process_template_dags","text":"Method to set child_process_task_templates for parallel execution Parameters: Name Type Description Default parallel_process_templates List of parallel process templates required","title":"set_parallel_process_template_dags()"},{"location":"api/#dagger.templates.template.IDynamicProcessTemplateDAG","text":"Bases: IProcessTemplateDAG Skeleton class defining the attributes and functions of dynamic processes and task instances.","title":"IDynamicProcessTemplateDAG"},{"location":"api/#dagger.templates.template.IDynamicProcessTemplateDAG.__init__","text":"Init method Parameters: Name Type Description Default next_process_dags List [ IProcessTemplateDAG ] The next Process to execute after the current process is complete required app Service The Dagger instance required name str The name of the process required max_run_duration int the timeout on the process required","title":"__init__()"},{"location":"api/#dagger.templates.template.IDynamicProcessTemplateDAG.create_instance","text":"Method for creating an dynamic instance(s) of a template and return the head of the list Parameters: Name Type Description Default id UUID The id of the instance to create required parent_id UUID the id of the parent instance required parent_name str the name of the parent workflow required partition_key_lookup str The kafka partitioning key to use serialize the storage of this instance required repartition bool If true, the instance is stored on the node owning the parition defined by the partioning key True seed random . Random the seed to use any create any child instances None workflow_instance ITemplateDAGInstance the instance of the workflow None **kwargs Other keyword arguments {}","title":"create_instance()"},{"location":"api/#dagger.templates.template.TaskTemplate","text":"Skeleton class defining the attributes and functions of process and task instances.","title":"TaskTemplate"},{"location":"api/#dagger.templates.template.TaskTemplate.create_instance","text":"Method for creating an instance of a template definition Parameters: Name Type Description Default id UUID the ID of the instance required parent_id UUID the ID of the parent instance if any required parent_name str the name of the parent instance required partition_key_lookup str The kafka paritioning key required repartition bool If true the instance is serialized and stored by the node owning the parition defined by the partitioning key True seed random . Random the seed to use to create any child instanes None workflow_instance ITemplateDAGInstance the workflow object None **kwargs Other keyword arguments {}","title":"create_instance()"},{"location":"api/#dagger.templates.template.DefaultTaskTemplate","text":"Bases: TaskTemplate Default implementation of task template.","title":"DefaultTaskTemplate"},{"location":"api/#dagger.templates.template.DefaultTaskTemplate.__init__","text":"Init method Parameters: Name Type Description Default app Service The Dagger Instance required type Type [ ITask ] The type of the task defined within the workflow required name str the name of the task required task_dag_template List [ TaskTemplate ] The next task to execute after this task required allow_skip_to bool Set to true if processing of the worklow can skip to this task required reprocess_on_message bool the task is executed when invoked irresepective of the state of the task False","title":"__init__()"},{"location":"api/#dagger.templates.template.DefaultTaskTemplate.create_instance","text":"Creates an instance of the task defined by this template Parameters: Name Type Description Default id UUID the id of the instance to create required parent_id UUID the id of the parent of this task to be created required parent_name str the name of the task of the parent of the task to be created required partition_key_lookup str the kafka partioning key if this instance needs to be repartitioned required repartition bool If true, the instance is stored in the node owning the partition defined by the paritioning key True seed random . Random the seed to use to create any child instances None workflow_instance ITemplateDAGInstance the workflow object None **kwargs Any other keywork arguments {}","title":"create_instance()"},{"location":"api/#dagger.templates.template.DefaultKafkaTaskTemplate","text":"Bases: DefaultTaskTemplate Default implementation of KafkaCommandTask.","title":"DefaultKafkaTaskTemplate"},{"location":"api/#dagger.templates.template.DefaultKafkaTaskTemplate.__init__","text":"Init method Parameters: Name Type Description Default app Service Dagger instance required type Type [ KafkaCommandTask [ KT , VT ]] The type of KafkaCommandTask required name str the name of the task required topic Topic The topic to send the command on required task_dag_templates List [ TaskTemplate ] the next task to execute after this task required allow_skip_to bool If true the execution of the workflow can jump to this task required reprocess_on_message bool if true, the task is executed irrespective of the state of this task False","title":"__init__()"},{"location":"api/#dagger.templates.template.DefaultKafkaTaskTemplate.create_instance","text":"Method for creating an instance of a kafkaTask Parameters: Name Type Description Default id UUID The id of the instance required parent_id UUID the id of the parent of this process(The workflow instance) required parent_name str the name of the workflow required partition_key_lookup str The kafka partitioning key to look up required repartition bool If true the instance is serialized and stored in the owner of the parition owned by the partioning key True seed random . Random The seed to use to create any child instances None workflow_instance ITemplateDAGInstance the instance of the workflow None **kwargs Any Other keyword arguments {} Returns: Type Description ITask the instance of the Process","title":"create_instance()"},{"location":"api/#dagger.templates.template.DefaultTriggerTaskTemplate","text":"Bases: DefaultTaskTemplate Default implementation of TriggerTask.","title":"DefaultTriggerTaskTemplate"},{"location":"api/#dagger.templates.template.DefaultTriggerTaskTemplate.__init__","text":"Init method Parameters: Name Type Description Default app Service The dagger instance required type Type [ TriggerTask [ KT , VT ]] The type of TriggerTask required name str the name of the task required time_to_execute_key str the key lookup in runtime paramters to execute the trigger required allow_skip_to bool Flag For skipping serial execution required","title":"__init__()"},{"location":"api/#dagger.templates.template.DefaultTriggerTaskTemplate.create_instance","text":"Method for creating an instance of a TriggerTask Parameters: Name Type Description Default id UUID The id of the instance required parent_id UUID the id of the parent of this process(The workflow instance) required parent_name str the name of the workflow required partition_key_lookup str The kafka partitioning key to look up required repartition bool If true the instance is serialized and stored in the owner of the parition owned by the partioning key True seed random . Random The seed to use to create any child instances None workflow_instance ITemplateDAGInstance the instance of the workflow None **kwargs Any Other keyword arguments {} Returns: Type Description ITask the instance of the Process","title":"create_instance()"},{"location":"api/#dagger.templates.template.DefaultIntervalTaskTemplate","text":"Bases: DefaultTaskTemplate Default implementation of IntervalTask.","title":"DefaultIntervalTaskTemplate"},{"location":"api/#dagger.templates.template.DefaultIntervalTaskTemplate.__init__","text":"Init method Parameters: Name Type Description Default app Service Dagger instance required type Type [ IntervalTask [ KT , VT ]] The type of IntervalTask required name str the name of the task required time_to_execute_key str the key to lookup in the runtime paramters to trigger the task required time_to_force_complete_key str the key to lookup to timeout the task required interval_execute_period_key str the frequency of execution from trigger time to timeout until the task succeeds required task_dag_templates List [ TaskTemplate ] the next task to execute required allow_skip_to bool Flag to skip serial execution required","title":"__init__()"},{"location":"api/#dagger.templates.template.DefaultIntervalTaskTemplate.create_instance","text":"Method for creating an instance of IntervalTask Parameters: Name Type Description Default id UUID The id of the instance required parent_id UUID the id of the parent of this process(The workflow instance) required parent_name str the name of the workflow required partition_key_lookup str The kafka partitioning key to look up required repartition bool If true the instance is serialized and stored in the owner of the parition owned by the partioning key True seed random . Random The seed to use to create any child instances None workflow_instance ITemplateDAGInstance the instance of the workflow None **kwargs Other keyword arguments {} Returns: Type Description ITask the instance of the Process","title":"create_instance()"},{"location":"api/#dagger.templates.template.ParallelCompositeTaskTemplate","text":"Bases: DefaultTaskTemplate Template to define the structure of parallel tasks to execute within a workflow","title":"ParallelCompositeTaskTemplate"},{"location":"api/#dagger.templates.template.ParallelCompositeTaskTemplate.__init__","text":"Init method Parameters: Name Type Description Default app Service The dagger instance required type Type [ ITask ] The type of ParallelCompositeTask required name str The name of the ParallelCompositeTask required task_dag_templates List [ TaskTemplate ] the next task to execute after the parallel task completes required child_task_templates List [ TaskTemplate ] the set of parallel tasks to execute required allow_skip_to bool Skip serial execution of the workflow if this is set required reprocess_on_message bool Re-execute the task irrespective of the state of the task False parallel_operator_type TaskOperator Wait for all parallel tasks to complete or just one before transitioning to the next task in the workflow TaskOperator.JOIN_ALL","title":"__init__()"},{"location":"api/#dagger.templates.template.DefaultTaskTemplateBuilder","text":"Bases: TaskTemplateBuilder Default Implementation of TaskTemplateBuilder","title":"DefaultTaskTemplateBuilder"},{"location":"api/#dagger.templates.template.ParallelCompositeTaskTemplateBuilder","text":"Bases: DefaultTaskTemplateBuilder A type of DefaultTaskTemplateBuilder to create ParallelTasks","title":"ParallelCompositeTaskTemplateBuilder"},{"location":"api/#dagger.templates.template.KafkaCommandTaskTemplateBuilder","text":"Bases: DefaultTaskTemplateBuilder A type of DefaultTaskTemplateBuilder to define KafkaCommandTasks","title":"KafkaCommandTaskTemplateBuilder"},{"location":"api/#dagger.templates.template.DecisionTaskTemplateBuilder","text":"Bases: DefaultTaskTemplateBuilder A type of DefaultTaskTemplateBuilder to define DecisionTasks","title":"DecisionTaskTemplateBuilder"},{"location":"api/#dagger.templates.template.TriggerTaskTemplateBuilder","text":"Bases: DefaultTaskTemplateBuilder A type of DefaultTaskTemplateBuilder to define TriggerTasks","title":"TriggerTaskTemplateBuilder"},{"location":"api/#dagger.templates.template.IntervalTaskTemplateBuilder","text":"Bases: TriggerTaskTemplateBuilder A type of DefaultTaskTemplateBuilder to define IntervalTasks","title":"IntervalTaskTemplateBuilder"},{"location":"api/#dagger.templates.template.KafkaListenerTaskTemplateBuilder","text":"Bases: DefaultTaskTemplateBuilder A type of DefaultTaskTemplateBuilder to define KafkaListenerTasks","title":"KafkaListenerTaskTemplateBuilder"},{"location":"api/#dagger.templates.template.KafkaListenerTaskTemplateBuilder.set_reprocess_on_message","text":"Set whether or not this task should reprocess on_message if a correlated message is reprocessed.","title":"set_reprocess_on_message()"},{"location":"api/#dagger.modeler.definition.DefaultTemplateBuilder","text":"Bases: ITemplateDAGBuilder Default implementation of template builder","title":"DefaultTemplateBuilder"},{"location":"api/#dagger.modeler.definition.DefaultTemplateBuilder.build","text":"Builds the DefaultTemplateBuilder Returns: Type Description ITemplateDAG the instance of ITemplateDAG from the builder definition","title":"build()"},{"location":"api/#dagger.modeler.definition.DefaultTemplateBuilder.set_name","text":"Sets the name of the task Parameters: Name Type Description Default name str The name of the task the builder is setting up required Returns: Type Description ITemplateDAGBuilder The updated ITemplateDAGBuilder","title":"set_name()"},{"location":"api/#dagger.modeler.definition.DefaultTemplateBuilder.set_root","text":"Sets the root process of this task Parameters: Name Type Description Default template IProcessTemplateDAG the parent Process Task required Returns: Type Description ITemplateDAGBuilder the instance of ITemplateDAGBuilder","title":"set_root()"},{"location":"api/#dagger.modeler.definition.DefaultTemplateBuilder.set_type","text":"Sets the type of the Task to instantiate from this builder Parameters: Name Type Description Default template_type Type [ ITemplateDAGInstance ] the Template Type required Returns: Type Description ITemplateDAGBuilder the instance of ITemplateDAGBuilder","title":"set_type()"},{"location":"api/#dagger.modeler.definition.ProcessTemplateDAG","text":"Bases: IProcessTemplateDAG Class that encapsulates the definition of a Process","title":"ProcessTemplateDAG"},{"location":"api/#dagger.modeler.definition.ProcessTemplateDAG.__init__","text":"Constructor Parameters: Name Type Description Default next_process_dag List [ IProcessTemplateDAG ] The list of next processes to execute in this workflow required app Dagger The Dagger instance required name str the name of the process required process_type Type [ ITask [ KT , VT ]] the type of the process to instantiate required root_task_dag Optional [ TaskTemplate ] The workflow type instance(definition) required max_run_duration int The maximum time this process can execute until its marked as failure(timeout) required","title":"__init__()"},{"location":"api/#dagger.modeler.definition.ProcessTemplateDAG.create_instance","text":"Method for creating an instance of a process instance Parameters: Name Type Description Default id UUID The id of the instance required parent_id UUID the id of the parent of this process(The workflow instance) required parent_name str the name of the workflow required partition_key_lookup str The kafka partitioning key to look up required repartition bool If true the instance is serialized and stored in the owner of the parition owned by the partioning key True seed random . Random The seed to use to create any child instances None workflow_instance ITemplateDAGInstance the instance of the workflow None **kwargs Any Other keyword arguments {} Returns: Type Description IProcessTemplateDAGInstance [ KT , VT ] the instance of the Process","title":"create_instance()"},{"location":"api/#dagger.modeler.definition.ParallelCompositeProcessTemplateDAG","text":"Bases: ProcessTemplateDAG A Process Template to define a set of parallel Processes within a Process","title":"ParallelCompositeProcessTemplateDAG"},{"location":"api/#dagger.modeler.definition.ParallelCompositeProcessTemplateDAG.__init__","text":"init method Parameters: Name Type Description Default next_process_dag List [ IProcessTemplateDAG ] The next Process in the workflow definition required app Dagger The Dagger instance required name str The name of the ParallelCompositeProcessTask required process_type Type [ ParallelCompositeTask [ KT , VT ]] The type of ParallelCompositeTask to be instantiated from the definition required child_process_task_templates List [ IProcessTemplateDAG ] The list of parallel processes to be created required parallel_operator_type TaskOperator Wait for either all to complete or just one before transitioning to the next process in the workflow required","title":"__init__()"},{"location":"api/#dagger.modeler.definition.ParallelCompositeProcessTemplateDAG.create_instance","text":"Create a ParallelCompositeTask instance based on the template definition Parameters: Name Type Description Default id UUID The id of the instance required parent_id UUID the id of the parent of this process(The workflow instance) required parent_name str the name of the workflow required partition_key_lookup str The kafka partitioning key to look up required repartition bool If true the instance is serialized and stored in the owner of the parition owned by the partioning key True seed random . Random The seed to use to create any child instances None workflow_instance ITemplateDAGInstance the instance of the workflow None **kwargs Any Other keyword arguments {} Returns: Type Description ParallelCompositeTask [ KT , VT ] the instance of the Process","title":"create_instance()"},{"location":"api/#dagger.modeler.definition.ParallelCompositeProcessTemplateDAG.set_parallel_process_template_dags","text":"Sets child_process_task_templates Parameters: Name Type Description Default parallel_process_templates List [ IProcessTemplateDAG ] List of parallel process templates required","title":"set_parallel_process_template_dags()"},{"location":"api/#dagger.modeler.definition.TemplateDAG","text":"Bases: ITemplateDAG Default Implementation of ITemplateDAG","title":"TemplateDAG"},{"location":"api/#dagger.modeler.definition.TemplateDAG.__init__","text":"Constructor Parameters: Name Type Description Default dag IProcessTemplateDAG The definition of the first process to execute required name str the name of the Workflow required app Dagger the Dagger instance required template_type Type [ ITemplateDAGInstance [ KT , VT ]] the type of the Workflow to instantiate required","title":"__init__()"},{"location":"api/#dagger.modeler.definition.TemplateDAG.create_instance","text":"Method for creating an instance of a workflow definition Parameters: Name Type Description Default id UUID The id of the workflow required partition_key_lookup str Kafka topic partition key associated with the instance of the workflow. The key needs to be defined within the runtime parameters required repartition bool Flag indicating if the creation of this instance needs to be stored on the current node or by the owner of the partition defined by the partition_key_lookup True seed random . Random the seed to use to create all internal instances of the workflow None **kwargs Other keyword arguments {} Returns: Type Description ITemplateDAGInstance [ KT , VT ] An instance of the workflow","title":"create_instance()"},{"location":"api/#dagger.modeler.definition.TemplateDAG.get_given_process","text":"Looks up for a specific process template within a DAG Parameters: Name Type Description Default process_name str Name of the process required Returns: Type Description Optional [ IProcessTemplateDAG ] Process Template if found, else None","title":"get_given_process()"},{"location":"api/#dagger.modeler.definition.TemplateDAG.set_given_num_of_parallel_processes_for_a_composite_process","text":"This method creates and sets 'N' number of new parralel processes for a given process in a DAG Parameters: Name Type Description Default no_of_processes int Number of parallel processes required required composite_process_name str Name of the process with in which the parallel processes must reside required parallel_template_builder ProcessTemplateDagBuilder A process template builder which needs to be cloned 'N' times and executed in parallel required","title":"set_given_num_of_parallel_processes_for_a_composite_process()"},{"location":"api/#dagger.modeler.definition.TemplateDAG.set_parallel_process_template_dags_for_a_composite_process","text":"Sets new process builders within a given process in a DAG Parameters: Name Type Description Default name str Name of the process with in which given process builders must reside required parallel_process_templates List [ IProcessTemplateDAG ] List of process template builders required","title":"set_parallel_process_template_dags_for_a_composite_process()"},{"location":"api/#dagger.modeler.definition.DynamicProcessTemplateDAG","text":"Bases: IDynamicProcessTemplateDAG A template to add Dynamic Processes to a workflow at runtime","title":"DynamicProcessTemplateDAG"},{"location":"api/#dagger.modeler.definition.DynamicProcessTemplateDAG.__init__","text":"Init method Parameters: Name Type Description Default next_process_dag List [ IProcessTemplateDAG ] the next process in the workflow definition required app Dagger The Dagger instance required name str the name of the Dynamic Process required max_run_duration int The timeout on the process to COMPLETE execution required","title":"__init__()"},{"location":"api/#dagger.modeler.definition.DynamicProcessTemplateDAG.create_instance","text":"Create a ParallelCompositeTask instance based on the template definition Parameters: Name Type Description Default id UUID The id of the instance required parent_id UUID the id of the parent of this process(The workflow instance) required parent_name str the name of the workflow required partition_key_lookup str The kafka partitioning key to look up required repartition bool If true the instance is serialized and stored in the owner of the parition owned by the partioning key True seed random . Random The seed to use to create any child instances None workflow_instance ITemplateDAGInstance the instance of the workflow None **kwargs Any Other keyword arguments {} Returns: Type Description IProcessTemplateDAGInstance [ KT , VT ] the instance of the Process","title":"create_instance()"},{"location":"api/#dagger.modeler.definition.ProcessTemplateDagBuilder","text":"Bases: IProcessTemplateDAGBuilder Default implementation of process builder","title":"ProcessTemplateDagBuilder"},{"location":"api/#dagger.modeler.definition.ProcessTemplateDagBuilder.__init__","text":"Init method Parameters: Name Type Description Default app Dagger The Dagger instance required","title":"__init__()"},{"location":"api/#dagger.modeler.definition.DynamicProcessTemplateDagBuilder","text":"Bases: IProcessTemplateDAGBuilder Skeleton builder class used to build a dynamic process object(s).","title":"DynamicProcessTemplateDagBuilder"},{"location":"api/#dagger.modeler.definition.DynamicProcessTemplateDagBuilder.set_root_task","text":"Not implemented. Raises: Type Description NotImplementedError Not implemented.","title":"set_root_task()"},{"location":"api/#dagger.modeler.definition.DynamicProcessTemplateDagBuilder.set_type","text":"Not implemented. Raises: Type Description NotImplementedError Not implemented.","title":"set_type()"},{"location":"api/#dagger.modeler.definition.ParallelCompositeProcessTemplateDagBuilder","text":"Bases: IProcessTemplateDAGBuilder Skeleton builder class used to build a parallel process object(s).","title":"ParallelCompositeProcessTemplateDagBuilder"},{"location":"api/#dagger.modeler.definition.ParallelCompositeProcessTemplateDagBuilder.set_max_run_duration","text":"Not implemented. Raises: Type Description NotImplementedError Not implemented.","title":"set_max_run_duration()"},{"location":"api/#dagger.modeler.definition.ParallelCompositeProcessTemplateDagBuilder.set_root_task","text":"Not implemented. Raises: Type Description NotImplementedError Not implemented.","title":"set_root_task()"},{"location":"api/#dagger.tasks.task.TaskStatusEnum","text":"Bases: Enum Class to indicate State of the Task","title":"TaskStatusEnum"},{"location":"api/#dagger.tasks.task.TaskStatusEnum.COMPLETED","text":"The Task COMPLETED Execution","title":"COMPLETED"},{"location":"api/#dagger.tasks.task.TaskStatusEnum.EXECUTING","text":"The task is currently EXECUTING","title":"EXECUTING"},{"location":"api/#dagger.tasks.task.TaskStatusEnum.FAILURE","text":"The Task Failed during Execution","title":"FAILURE"},{"location":"api/#dagger.tasks.task.TaskStatusEnum.NOT_STARTED","text":"The Task has NOT STARTED EXECUTION","title":"NOT_STARTED"},{"location":"api/#dagger.tasks.task.TaskStatusEnum.SKIPPED","text":"The Task Skipped Execution","title":"SKIPPED"},{"location":"api/#dagger.tasks.task.TaskStatusEnum.STOPPED","text":"The Task execution was STOPPED","title":"STOPPED"},{"location":"api/#dagger.tasks.task.TaskStatusEnum.SUBMITTED","text":"The Task was SUBMITTED for Execution","title":"SUBMITTED"},{"location":"api/#dagger.tasks.task.TaskType","text":"Bases: Enum The type of the Task","title":"TaskType"},{"location":"api/#dagger.tasks.task.TaskType.LEAF","text":"Task which has no children","title":"LEAF"},{"location":"api/#dagger.tasks.task.TaskType.PARALLEL_COMPOSITE","text":"A container for parallel tasks","title":"PARALLEL_COMPOSITE"},{"location":"api/#dagger.tasks.task.TaskType.ROOT","text":"The Root node of the workflow","title":"ROOT"},{"location":"api/#dagger.tasks.task.TaskType.SUB_DAG","text":"A Process Task that consists of LEAF Tasks","title":"SUB_DAG"},{"location":"api/#dagger.tasks.task.TaskStatus","text":"Bases: Record Class to serialize the status of the Task","title":"TaskStatus"},{"location":"api/#dagger.tasks.task.ITask","text":"Bases: Record , Generic [ KT , VT ] Class that every template, process, and task extends. Defines attributes and core functions that Dagger uses.","title":"ITask"},{"location":"api/#dagger.tasks.task.ITask.evaluate","text":"Processes some inputs and determines the next ITask id. Returns: Type Description Optional [ UUID ] The next ITask id.","title":"evaluate()"},{"location":"api/#dagger.tasks.task.ITask.execute","text":"Executes the ITask. Parameters: Name Type Description Default workflow_instance ITask The workflow object None","title":"execute()"},{"location":"api/#dagger.tasks.task.ITask.get_correlatable_key","text":"Get the lookup key,value associated with the task.Deprecated use get_correlatable_key_from_payload Parameters: Name Type Description Default payload Any The lookup key,value. required Returns: Type Description TaskLookupKey used to associate a task with a message.","title":"get_correlatable_key()"},{"location":"api/#dagger.tasks.task.ITask.get_correlatable_key_from_payload","text":"Get the lookup key,value associated with the task(Deprecated use get_correlatable_keys_from_payload). Parameters: Name Type Description Default payload Any The lookup key,value. required Returns: Type Description TaskLookupKey used to associate a task with a message.","title":"get_correlatable_key_from_payload()"},{"location":"api/#dagger.tasks.task.ITask.get_correlatable_keys_from_payload","text":"Get a list of lookup key,value associated with the task(s). Parameters: Name Type Description Default payload Any The lookup key,value. required Returns: Type Description List [ TaskLookupKey ] used to associate a tasks with a message.","title":"get_correlatable_keys_from_payload()"},{"location":"api/#dagger.tasks.task.ITask.get_remaining_tasks","text":"Get the remaining tasks in the workflow. Parameters: Name Type Description Default next_dag_id UUID Current ITask id. required tasks Optional [ List [ ITask ]] List of previous ITasks. Defaults to []. None end_task_id UUID The task id that the function should stop and return at. Defaults to None (so end of DAG). None workflow_instance ITemplateDAGInstance The Workflow object required Returns: Type Description Optional [ List [ ITask ]] List of remaining ITasks appended to inputted list.","title":"get_remaining_tasks()"},{"location":"api/#dagger.tasks.task.ITask.notify","text":"If not completed, runs the steps required for completion by calling on_complete(). This is used to signal a task that it can now complete Parameters: Name Type Description Default status TaskStatus the status of the task to set to when completed required workflow_instance Optional [ ITemplateDAGInstance ] the Workflow object required","title":"notify()"},{"location":"api/#dagger.tasks.task.ITask.on_complete","text":"Sets the status of the ITask to completed and starts the next ITask if there is one. Parameters: Name Type Description Default workflow_instance Optional [ ITemplateDAGInstance ] The workflow object required status TaskStatus The status of the task to set to TaskStatus(code=TaskStatusEnum.COMPLETED.name, value=TaskStatusEnum.COMPLETED.value)","title":"on_complete()"},{"location":"api/#dagger.tasks.task.ITask.on_message","text":"Defines what to do when the task recieves a message. Parameters: Name Type Description Default runtime_parameters Dict [ str , str ] The runtime parameters of the task required Returns: Type Description bool True if the processing succeeds false otherwise","title":"on_message()"},{"location":"api/#dagger.tasks.task.ITask.start","text":"Starts the ITask. Parameters: Name Type Description Default workflow_instance Optional [ ITemplateDAGInstance ] The Workflow instance required","title":"start()"},{"location":"api/#dagger.tasks.task.ITask.stop","text":"Stops the ITask.","title":"stop()"},{"location":"api/#dagger.tasks.task.MonitoringTask","text":"Bases: TriggerTask [ KT , VT ] , abc . ABC A Type of TriggerTask that executes at s specific time and checks on the monitored task to execute some domain specific logic","title":"MonitoringTask"},{"location":"api/#dagger.tasks.task.MonitoringTask.process_monitored_task","text":"Callback on when business logic has to be executed on the monitored task based on the time condition Parameters: Name Type Description Default monitored_task ITask the monitored task required workflow_instance Optional [ ITemplateDAGInstance ] the workflow object required Returns: Type Description None None","title":"process_monitored_task()"},{"location":"api/#dagger.tasks.task.IntervalTask","text":"Bases: TriggerTask [ KT , VT ] , abc . ABC A type of Task to Trigger at a trigger time and execute multiple times until the execution completes. The task is retried until the timeout is reached periodically after the trigger time","title":"IntervalTask"},{"location":"api/#dagger.tasks.task.IntervalTask.interval_execute","text":"Task to run on an interval until either the trigger end time or until this method returns True. Parameters: Name Type Description Default runtime_parameters Dict [ str , VT ] The runtime parameters of the task required Returns: Type Description bool If True, finish this task.","title":"interval_execute()"},{"location":"api/#dagger.tasks.task.TriggerTask","text":"Bases: ExecutorTask [ KT , VT ] , abc . ABC This task waits/halts the execution of the DAG until current time >= the trigger time on the task and then invokes the execute method defined by the task","title":"TriggerTask"},{"location":"api/#dagger.tasks.task.ExecutorTask","text":"Bases: ITask [ KT , VT ] , abc . ABC A simple ITask that executes some domain specific logic","title":"ExecutorTask"},{"location":"api/#dagger.tasks.task.ExecutorTask.evaluate","text":"Not implemented. Raises: Type Description NotImplementedError Not implemented.","title":"evaluate()"},{"location":"api/#dagger.tasks.task.ExecutorTask.on_message","text":"Not implemented. Raises: Type Description NotImplementedError Not implemented.","title":"on_message()"},{"location":"api/#dagger.tasks.task.DefaultMonitoringTask","text":"Bases: MonitoringTask [ str , str ] Default Implementation of MonitoringTask","title":"DefaultMonitoringTask"},{"location":"api/#dagger.tasks.task.DecisionTask","text":"Bases: ITask [ KT , VT ] This type of task is similar to the case..switch statement in a programming language. It returns the next task to execute based on the execution logic. A decision task needs to implement","title":"DecisionTask"},{"location":"api/#dagger.tasks.task.DecisionTask.execute","text":"Not implemented. Raises: NotImplementedError: Not implemented.","title":"execute()"},{"location":"api/#dagger.tasks.task.DecisionTask.on_message","text":"Not implemented. Raises: NotImplementedError: Not implemented.","title":"on_message()"},{"location":"api/#dagger.tasks.task.SystemTask","text":"Bases: ExecutorTask [ str , str ] An internal Task for Dagger bookkeeping","title":"SystemTask"},{"location":"api/#dagger.tasks.task.SystemTask.evaluate","text":"Not implemented. Raises: Type Description NotImplementedError Not implemented.","title":"evaluate()"},{"location":"api/#dagger.tasks.task.SystemTask.get_correlatable_key","text":"Not implemented. Raises: Type Description NotImplementedError Not implemented.","title":"get_correlatable_key()"},{"location":"api/#dagger.tasks.task.SystemTask.on_complete","text":"Not implemented. Raises: Type Description NotImplementedError Not implemented.","title":"on_complete()"},{"location":"api/#dagger.tasks.task.SystemTask.on_message","text":"Not implemented. Raises: Type Description NotImplementedError Not implemented.","title":"on_message()"},{"location":"api/#dagger.tasks.task.SystemTimerTask","text":"Bases: SystemTask A type of SystemTask to execute internal Dagger Tasks","title":"SystemTimerTask"},{"location":"api/#dagger.tasks.task.SensorTask","text":"Bases: ITask [ KT , VT ] , abc . ABC A type of task that halts execution of the workflow until a condition is met. When the condition is met the on_message method on this task is invoked","title":"SensorTask"},{"location":"api/#dagger.tasks.task.SensorTask.evaluate","text":"Not implemented. Raises: Type Description NotImplementedError Not implemented.","title":"evaluate()"},{"location":"api/#dagger.tasks.task.SensorTask.execute","text":"Not implemented. Raises: Type Description NotImplementedError Not implemented.","title":"execute()"},{"location":"api/#dagger.tasks.task.IMonitoredTask","text":"Abstract interface to enable monitoring of a task","title":"IMonitoredTask"},{"location":"api/#dagger.tasks.task.IMonitoredTask.get_monitoring_task_type","text":"Get the TaskType to instantiate to monitor the current task Returns: Type Description Type [ MonitoringTask ] The Type of MonitoringTask","title":"get_monitoring_task_type()"},{"location":"api/#dagger.tasks.task.KafkaCommandTask","text":"Bases: ExecutorTask [ KT , VT ] , abc . ABC This task is used to send a request/message on a Kafka Topic defined using the template builder. This type of task is a child task in the execution graph and can be extended by implementing the method","title":"KafkaCommandTask"},{"location":"api/#dagger.tasks.task.KafkaListenerTask","text":"Bases: SensorTask [ KT , VT ] , abc . ABC This task waits/halts the execution of the DAG until a message is received on the defined Kafka topic(in the template definition). Each task is created using the DAG builder defines a durable key to correlate each received message on the topic against listener tasks. The Engine handles the complexity of invoking the appropriate task instance based on the key in the payload.","title":"KafkaListenerTask"},{"location":"api/#dagger.tasks.task.INonLeafNodeTask","text":"Bases: ITask [ KT , VT ] , abc . ABC An Abstract class for any Process/SUB_DAG node","title":"INonLeafNodeTask"},{"location":"api/#dagger.tasks.task.TaskOperator","text":"Bases: Enum An operator for Joining Parallel Tasks","title":"TaskOperator"},{"location":"api/#dagger.tasks.task.TaskOperator.ATLEAST_ONE","text":"Wait for Atleast one of the parallel tasks to reach terminal state to begin execution of the next task in the workflow definition","title":"ATLEAST_ONE"},{"location":"api/#dagger.tasks.task.TaskOperator.JOIN_ALL","text":"Waits for All the parallel tasks to reach terminal state before execution of the next task in the workflow definition","title":"JOIN_ALL"},{"location":"api/#dagger.tasks.task.ParallelCompositeTask","text":"Bases: ITask [ KT , VT ] , abc . ABC SUB-DAG Task to execute parallel tasks and wait until all of them are in a terminal state before progressing to the next task This task can be embedded as a child of the root node or a process node","title":"ParallelCompositeTask"},{"location":"api/#dagger.tasks.task.ParallelCompositeTask.notify","text":"If not completed, runs the steps required for completion by calling on_complete().","title":"notify()"},{"location":"api/#dagger.tasks.task.IProcessTemplateDAGInstance","text":"Bases: INonLeafNodeTask [ KT , VT ] , abc . ABC A Process implementation of INonLeafNodeTask","title":"IProcessTemplateDAGInstance"},{"location":"api/#dagger.tasks.task.CorrelatableMapValue","text":"Bases: Record An internal Class to store the correletable keys and their associated values for SensorTask","title":"CorrelatableMapValue"},{"location":"api/#dagger.tasks.task.ITemplateDAGInstance","text":"Bases: INonLeafNodeTask [ KT , VT ] , abc . ABC A root node implementation of INonLeafNodeTask","title":"ITemplateDAGInstance"},{"location":"api/#dagger.tasks.task.MonitoredProcessTemplateDAGInstance","text":"Bases: DefaultProcessTemplateDAGInstance , IMonitoredTask Default implementation of a Monitored ProcessTask","title":"MonitoredProcessTemplateDAGInstance"},{"location":"api/#dagger.tasks.task.MonitoredProcessTemplateDAGInstance.on_complete","text":"Sets the status of the ITask to completed and starts the next ITask if there is one.","title":"on_complete()"},{"location":"api/#dagger.tasks.task.Trigger","text":"Bases: Record Class To store the Trigger data. The time to execute a task","title":"Trigger"},{"location":"api/#dagger.tasks.task.Trigger.get_trigger_key","text":"The key to store for the trigger instance Returns: Type Description Tuple [ Optional [ UUID ], Optional [ UUID ]] The Key to store","title":"get_trigger_key()"},{"location":"api/#dagger.exceptions.exceptions.DaggerError","text":"Bases: Exception Base-class for all Dagger exceptions.","title":"DaggerError"},{"location":"api/#dagger.exceptions.exceptions.InvalidTaskType","text":"Bases: DaggerError Invalid Task Type.","title":"InvalidTaskType"},{"location":"api/#dagger.exceptions.exceptions.TaskInvalidState","text":"Bases: DaggerError Invalid Task State.","title":"TaskInvalidState"},{"location":"api/#dagger.exceptions.exceptions.TemplateDoesNotExist","text":"Bases: DaggerError Invalid Template Name","title":"TemplateDoesNotExist"},{"location":"api/#dagger.exceptions.exceptions.InvalidTriggerTimeForTask","text":"Bases: DaggerError Invalid trigger time","title":"InvalidTriggerTimeForTask"},{"location":"development-guide/","text":"Development Guide Welcome! Thank you for wanting to make the project better. This section provides an overview of repository structure and how to work with the code base. Before you dive into this, it is best to read: The whole Usage Guide The Code of Conduct The Contributing guide Docker The Dagger project uses Docker to ease setting up a consistent development environment. The Docker documentation has details on how to install docker on your computer. Once that is configured, the integration test suite can be run locally: docker-compose run --rm integation_test Testing You'll be unable to merge code unless the linting and tests pass. You can run these in your container via: docker-compose run --rm test This will run the same tests, linting, and code coverage that are run by the CI pipeline. The only difference is that, when run locally, black and isort are configured to automatically correct issues they detect. Generally we should endeavor to write tests for every feature. Every new feature branch should increase the test coverage rather than decreasing it. We use pytest as our testing framework. Stages To customize / override a specific testing stage, please read the documentation specific to that tool: PyTest MyPy Black Isort Flake8 Bandit Building the Library dagger is PEP 517 compliant. build is used as the frontend tool for building the library. Setuptools is used as the build backend. setup.cfg contains the library metadata. A setup.py is also included to support an editable install. Requirements requirements.txt - Lists all direct dependencies (packages imported by the library). requirements-test.txt - Lists all direct dependencies needed for development. This primarily covers dependencies needed to run the test suite & lints. Publishing a New Version Once the package is ready to be released, there are a few things that need to be done: Start with a local clone of the repo on the default branch with a clean working tree. Run the version bump script with the appropriate part name ( major , minor , or patch ). Example: docker-compose run --rm bump minor This wil create a new branch, updates all affected files with the new version, and commit the changes to the branch. Push the new branch to create a new pull request. Get the pull request approved. Merge the pull request to the default branch. Merging the pull request will trigger a GitHub Action that will create a new release. The creation of this new release will trigger a GitHub Action that will to build a wheel & a source distributions of the package and push them to PyPI . Warning The action that uploads the files to PyPI will not run until a repository maintainer acknowledges that the job is ready to run. This is to keep the PyPI publishing token secure. Otherwise, any job would have access to the token. In addition to uploading the files to PyPI, the documentation website will be updated to include the new version. If the new version is a full release, it will be made the new latest version. Continuous Integration Pipeline The Continuous Integration (CI) Pipeline runs to confirm that the repository is in a good state. It will run when someone creates a pull request or when they push new commits to the branch for an existing pull request. The pipeline runs multiple different jobs that helps verify the state of the code. This same pipeline also runs on the default branch when a maintainer merges a pull request. Lints The first set of jobs that run as part of the CI pipline are linters that perform static analysis on the code. This includes: MyPy , Black , Isort , Flake8 , and Bandit . Tests The next set of jobs run the unit tests using PyTest . The pipeline runs the tests cases across each supported version of Python to ensure compatibility. For each run of the test cases, the job will record the test results and code coverage information. The pipeline uploads the code coverage information to CodeCov to ensure that a pull request doesn't significantly reduce the total code coverage percentage or introduce a large amount of code that is untested. Distribution Verification The next set of jobs build the wheel distribution, installs in into a virtual environment, and then runs Python to import the library version. This works as a smoke test to ensure that the library can be packaged correctly and used. The pipeline runs the tests cases across each supported version of Python to ensure compatibility. Documentation The remaining jobs are all related to documentation. A job builds the documentation in strict mode so that it will fail if there are any errors. The job records the generated files so that the documentation website can be viewed in its rendered form. When the pipeline is running as a result of a maintainer merging a pull request to the default branch, a job runs that publishes the current state of the documentation to as the dev version. This will allow users to view the state of the documentation as it has changed since a maintainer published the latest version.","title":"Development Guide"},{"location":"development-guide/#development-guide","text":"Welcome! Thank you for wanting to make the project better. This section provides an overview of repository structure and how to work with the code base. Before you dive into this, it is best to read: The whole Usage Guide The Code of Conduct The Contributing guide","title":"Development Guide"},{"location":"development-guide/#docker","text":"The Dagger project uses Docker to ease setting up a consistent development environment. The Docker documentation has details on how to install docker on your computer. Once that is configured, the integration test suite can be run locally: docker-compose run --rm integation_test","title":"Docker"},{"location":"development-guide/#testing","text":"You'll be unable to merge code unless the linting and tests pass. You can run these in your container via: docker-compose run --rm test This will run the same tests, linting, and code coverage that are run by the CI pipeline. The only difference is that, when run locally, black and isort are configured to automatically correct issues they detect. Generally we should endeavor to write tests for every feature. Every new feature branch should increase the test coverage rather than decreasing it. We use pytest as our testing framework.","title":"Testing"},{"location":"development-guide/#stages","text":"To customize / override a specific testing stage, please read the documentation specific to that tool: PyTest MyPy Black Isort Flake8 Bandit","title":"Stages"},{"location":"development-guide/#building-the-library","text":"dagger is PEP 517 compliant. build is used as the frontend tool for building the library. Setuptools is used as the build backend. setup.cfg contains the library metadata. A setup.py is also included to support an editable install.","title":"Building the Library"},{"location":"development-guide/#requirements","text":"requirements.txt - Lists all direct dependencies (packages imported by the library). requirements-test.txt - Lists all direct dependencies needed for development. This primarily covers dependencies needed to run the test suite & lints.","title":"Requirements"},{"location":"development-guide/#publishing-a-new-version","text":"Once the package is ready to be released, there are a few things that need to be done: Start with a local clone of the repo on the default branch with a clean working tree. Run the version bump script with the appropriate part name ( major , minor , or patch ). Example: docker-compose run --rm bump minor This wil create a new branch, updates all affected files with the new version, and commit the changes to the branch. Push the new branch to create a new pull request. Get the pull request approved. Merge the pull request to the default branch. Merging the pull request will trigger a GitHub Action that will create a new release. The creation of this new release will trigger a GitHub Action that will to build a wheel & a source distributions of the package and push them to PyPI . Warning The action that uploads the files to PyPI will not run until a repository maintainer acknowledges that the job is ready to run. This is to keep the PyPI publishing token secure. Otherwise, any job would have access to the token. In addition to uploading the files to PyPI, the documentation website will be updated to include the new version. If the new version is a full release, it will be made the new latest version.","title":"Publishing a New Version"},{"location":"development-guide/#continuous-integration-pipeline","text":"The Continuous Integration (CI) Pipeline runs to confirm that the repository is in a good state. It will run when someone creates a pull request or when they push new commits to the branch for an existing pull request. The pipeline runs multiple different jobs that helps verify the state of the code. This same pipeline also runs on the default branch when a maintainer merges a pull request.","title":"Continuous Integration Pipeline"},{"location":"development-guide/#lints","text":"The first set of jobs that run as part of the CI pipline are linters that perform static analysis on the code. This includes: MyPy , Black , Isort , Flake8 , and Bandit .","title":"Lints"},{"location":"development-guide/#tests","text":"The next set of jobs run the unit tests using PyTest . The pipeline runs the tests cases across each supported version of Python to ensure compatibility. For each run of the test cases, the job will record the test results and code coverage information. The pipeline uploads the code coverage information to CodeCov to ensure that a pull request doesn't significantly reduce the total code coverage percentage or introduce a large amount of code that is untested.","title":"Tests"},{"location":"development-guide/#distribution-verification","text":"The next set of jobs build the wheel distribution, installs in into a virtual environment, and then runs Python to import the library version. This works as a smoke test to ensure that the library can be packaged correctly and used. The pipeline runs the tests cases across each supported version of Python to ensure compatibility.","title":"Distribution Verification"},{"location":"development-guide/#documentation","text":"The remaining jobs are all related to documentation. A job builds the documentation in strict mode so that it will fail if there are any errors. The job records the generated files so that the documentation website can be viewed in its rendered form. When the pipeline is running as a result of a maintainer merging a pull request to the default branch, a job runs that publishes the current state of the documentation to as the dev version. This will allow users to view the state of the documentation as it has changed since a maintainer published the latest version.","title":"Documentation"},{"location":"getting-started/","text":"Getting Started Installation To install dagger , simply run this simple command in your terminal of choice: python -m pip install wf-dagger dagger has a dependency on faust-streaming for kafka stream processing Introduction The core of dagger are Tasks used to define workflows. Dagger uses faust-streaming to run asynchronous workflows and uses one faust-supported datastores to store the data. FAQ Which version of python is supported? dagger supports python version >= 3.7 What kafka versions are supported? dagger supports kafka with version >= 0.10. What's Next? Read the Usage Guide for a more detailed descriptions of ways you can use dagger . Read the API Reference for specific information about all the functions and classes made available by dagger .","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"","title":"Getting Started"},{"location":"getting-started/#installation","text":"To install dagger , simply run this simple command in your terminal of choice: python -m pip install wf-dagger dagger has a dependency on faust-streaming for kafka stream processing","title":"Installation"},{"location":"getting-started/#introduction","text":"The core of dagger are Tasks used to define workflows. Dagger uses faust-streaming to run asynchronous workflows and uses one faust-supported datastores to store the data.","title":"Introduction"},{"location":"getting-started/#faq","text":"","title":"FAQ"},{"location":"getting-started/#which-version-of-python-is-supported","text":"dagger supports python version >= 3.7","title":"Which version of python is supported?"},{"location":"getting-started/#what-kafka-versions-are-supported","text":"dagger supports kafka with version >= 0.10.","title":"What kafka versions are supported?"},{"location":"getting-started/#whats-next","text":"Read the Usage Guide for a more detailed descriptions of ways you can use dagger . Read the API Reference for specific information about all the functions and classes made available by dagger .","title":"What's Next?"},{"location":"why-dagger/","text":"Why Dagger? dagger started as an internal library used at Wayfair by the Fulfillment Engineering Team. It is now an open source project with the hope that it will provide benefit from the greater Python community. What problem did dagger initially solve? With our move to microservices, we chose orchestration over choreography to orchestrate between microservices. To achieve orchestration between microservices, the engineering team built dagger to model and execute workflows at scale. The reason why we chose to built dagger was Ability to modify runtime parameters after execution of the workflow Built as a lightweight library Highly available, Scalable and Fault Tolerant Supports kafka out of the box for asynchronous API's Supports long running tasks Simple deployment model - no DB setup and dependency Flexible model to support any kind of workflows Event sources all workflow updates Alternatives dagger is not the only library that exists which provides a way to codify prompting a user for answers to a set of questions. This section compares dagger with some frameworks which were created to achieve this task. Airflow Airflow is an workflow engine implemented in python. However it does not support long running tasks and has no integration with Kafka. It also uses a single RDBMS as a data store thus not giving us the scale out feature we were looking for Cadence cadence is another workflow engine with support for java and go. It however needs cassandra/mysql as the data store and does not have any kafka support built in. It does not offer a python client","title":"Why dagger?"},{"location":"why-dagger/#why-dagger","text":"dagger started as an internal library used at Wayfair by the Fulfillment Engineering Team. It is now an open source project with the hope that it will provide benefit from the greater Python community.","title":"Why Dagger?"},{"location":"why-dagger/#what-problem-did-dagger-initially-solve","text":"With our move to microservices, we chose orchestration over choreography to orchestrate between microservices. To achieve orchestration between microservices, the engineering team built dagger to model and execute workflows at scale. The reason why we chose to built dagger was Ability to modify runtime parameters after execution of the workflow Built as a lightweight library Highly available, Scalable and Fault Tolerant Supports kafka out of the box for asynchronous API's Supports long running tasks Simple deployment model - no DB setup and dependency Flexible model to support any kind of workflows Event sources all workflow updates","title":"What problem did dagger initially solve?"},{"location":"why-dagger/#alternatives","text":"dagger is not the only library that exists which provides a way to codify prompting a user for answers to a set of questions. This section compares dagger with some frameworks which were created to achieve this task.","title":"Alternatives"},{"location":"why-dagger/#airflow","text":"Airflow is an workflow engine implemented in python. However it does not support long running tasks and has no integration with Kafka. It also uses a single RDBMS as a data store thus not giving us the scale out feature we were looking for","title":"Airflow"},{"location":"why-dagger/#cadence","text":"cadence is another workflow engine with support for java and go. It however needs cassandra/mysql as the data store and does not have any kafka support built in. It does not offer a python client","title":"Cadence"},{"location":"usage-guide/fundamentals/","text":"Usage Guide This section provides detailed descriptions of how dagger can be used. If you are new to dagger , the Getting Started page provides a gradual introduction of the basic functionality with examples. Task Support Tasks are the building blocks to define workflows. dagger supports the following types of tasks: KafkaCommandTask This task is used to send a request/message on a Kafka Topic defined using the template builder. This type of task is a child task in the execution graph and can be extended by implementing the method @abc . abstractmethod async def execute ( self , runtime_parameters : Dict [ str , str ], workflow_instance : ITask = None ) -> None : KafkaListenerTask This task waits/halts the execution of the DAG until a message is received on the defined Kafka topic(in the template definition). Each task is created using the DAG builder defines a durable key to correlate each received message on the topic against listener tasks. The Engine handles the complexity of invoking the appropriate task instance based on the key in the payload. A listener task needs to implement the following methods @abc . abstractmethod async def on_message ( self , runtime_parameters : Dict [ Any , Any ], * args : Any , ** kwargs : Any ) -> bool : ... async def get_correlatable_key_from_payload ( self , payload : Any ) -> TaskLookupKey : The get_correlatable_key_from_payload method extracts the key value by parsing the payload received on the Kafka topic. Using this key dagger looks up the appropriate task from the list of tasks waiting on this event and invokes on_message on each one of them. The default implementation of this task just sets this task to COMPLETED dagger provides the flexibility to implement any other type of listener task by implementing the following interface class SensorTask ( ITask [ KT , VT ]): along with a custom TaskTemplateBuilder TriggerTask This task waits/halts the execution of the DAG until current time >= the trigger time on the task and then invokes the execute method defined by the task A trigger task needs to implement the following method @abc . abstractmethod async def execute ( self , runtime_parameters : Dict [ str , str ], workflow_instance : ITask = None ) -> None : \"\"\"Executes the ITask.\"\"\" ... dagger provides a TriggerTaskTemplateBuilder helper to model the task in the DAG. The set_time_to_execute_lookup_key on this builder is used to define the key to lookup the trigger time provided in the runtime parameters of the task DecisionTask This type of task is similar to the case..switch statement in a programming language. It returns the next task to execute based on the execution logic. A decision task needs to implement @abc . abstractmethod async def evaluate ( self , ** kwargs : Any ) -> Optional [ UUID ]: ... This method returns the UUID of the next task to execute in the execution path The Engine provides a DecisionTaskTemplateBuilder to model a decision task in the DAG MonitoredTask dagger provides the IMonitoredTask interface which can be implemented on any task to provide a way to monitor that task. When the monitor is triggered based on a trigger, the process_monitored_task method is invoked on the MonitoringTask . Dagger comes built in with MonitoredProcessTemplateDAGInstance ParallelCompositeTask SUB-DAG Task to execute parallel tasks and wait until all of them are in a terminal state before progressing to the next task This task can be embedded as a child of the root node or a process node IntervalTask A type of Task to Trigger at a trigger time and execute multiple times until the execution completes. The task is retried until the timeout is reached periodically after the trigger time MonitoringTask A Type of TriggerTask that executes at s specific time and checks on the monitored task to execute some domain specific logic SensorTask A type of task that halts execution of the workflow until a condition is met. When the condition is met the on_message method on this task is invoked IMonitoredTask An Abstract interface to enable monitoring of a task. Any Task that implements this interface will need to setup a MonitoringTask by implementing. Dagger ships with a default implementation using `` @abc . abstractmethod async def setup_monitoring_task ( self , workflow_instance : ITask ) -> None : ... RESTful API The framework provides a RESTFul API to retrieve the status of root task instances. Root task is the instance created using the TaskTemplate which then has multiple, chained ProcessTasks and child tasks(KafkaCommand and KafkaListener tasks) h tt p : //<hostname>:6066/tasks/instances [ { \"child_dags\" : [], \"child_tasks\" : [ { \"child_dags\" : [ \"89bbf26c-0727-11ea-96e5-0242ac150004\" , \"89bc1486-0727-11ea-96e5-0242ac150004\" ], \"correlatable_key\" : null , \"id\" : \"89bbedd0-0727-11ea-96e5-0242ac150004\" , \"lastupdated\" : 1573767727 , \"parent_id\" : \"89bbe43e-0727-11ea-96e5-0242ac150004\" , \"process_name\" : \"PAYMENT\" , \"runtime_parameters\" : { \"order_number\" : \"ID000\" , \"customer\" : \"ID000\" }, \"status\" : { \"code\" : \"COMPLETED\" , \"value\" : \"Complete\" }, \"task_type\" : \"NON_ROOT\" , \"time_completed\" : 1573767727 , \"time_created\" : 1573767624 , \"time_submitted\" : 1573767698 }, { \"child_dags\" : [ \"89bc3984-0727-11ea-96e5-0242ac150004\" , \"89bc482a-0727-11ea-96e5-0242ac150004\" ], \"correlatable_key\" : null , \"id\" : \"89bc35f6-0727-11ea-96e5-0242ac150004\" , \"lastupdated\" : 1573767727 , \"parent_id\" : \"89bbe43e-0727-11ea-96e5-0242ac150004\" , \"process_name\" : \"SHIPPING\" , \"runtime_parameters\" : { \"order_number\" : \"ID000\" , \"customer\" : \"ID000\" }, \"status\" : { \"code\" : \"EXECUTING\" , \"value\" : \"Executing\" }, \"task_type\" : \"NON_ROOT\" , \"time_completed\" : 0 , \"time_created\" : 1573767624 , \"time_submitted\" : 1573767727 } ], \"correlatable_key\" : null , \"id\" : \"89bbe43e-0727-11ea-96e5-0242ac150004\" , \"lastupdated\" : 1573767624 , \"parent_id\" : null , \"runtime_parameters\" : { \"order_number\" : \"ID000\" , \"customer\" : \"ID000\" }, \"status\" : { \"code\" : \"EXECUTING\" , \"value\" : \"Executing\" }, \"task_type\" : \"ROOT\" , \"time_completed\" : 0 , \"time_created\" : 1573767624 , \"time_submitted\" : 1573767698 }] Detailed Sections getting-started","title":"Fundamentals"},{"location":"usage-guide/fundamentals/#usage-guide","text":"This section provides detailed descriptions of how dagger can be used. If you are new to dagger , the Getting Started page provides a gradual introduction of the basic functionality with examples.","title":"Usage Guide"},{"location":"usage-guide/fundamentals/#task-support","text":"Tasks are the building blocks to define workflows. dagger supports the following types of tasks:","title":"Task Support"},{"location":"usage-guide/fundamentals/#kafkacommandtask","text":"This task is used to send a request/message on a Kafka Topic defined using the template builder. This type of task is a child task in the execution graph and can be extended by implementing the method @abc . abstractmethod async def execute ( self , runtime_parameters : Dict [ str , str ], workflow_instance : ITask = None ) -> None :","title":"KafkaCommandTask"},{"location":"usage-guide/fundamentals/#kafkalistenertask","text":"This task waits/halts the execution of the DAG until a message is received on the defined Kafka topic(in the template definition). Each task is created using the DAG builder defines a durable key to correlate each received message on the topic against listener tasks. The Engine handles the complexity of invoking the appropriate task instance based on the key in the payload. A listener task needs to implement the following methods @abc . abstractmethod async def on_message ( self , runtime_parameters : Dict [ Any , Any ], * args : Any , ** kwargs : Any ) -> bool : ... async def get_correlatable_key_from_payload ( self , payload : Any ) -> TaskLookupKey : The get_correlatable_key_from_payload method extracts the key value by parsing the payload received on the Kafka topic. Using this key dagger looks up the appropriate task from the list of tasks waiting on this event and invokes on_message on each one of them. The default implementation of this task just sets this task to COMPLETED dagger provides the flexibility to implement any other type of listener task by implementing the following interface class SensorTask ( ITask [ KT , VT ]): along with a custom TaskTemplateBuilder","title":"KafkaListenerTask"},{"location":"usage-guide/fundamentals/#triggertask","text":"This task waits/halts the execution of the DAG until current time >= the trigger time on the task and then invokes the execute method defined by the task A trigger task needs to implement the following method @abc . abstractmethod async def execute ( self , runtime_parameters : Dict [ str , str ], workflow_instance : ITask = None ) -> None : \"\"\"Executes the ITask.\"\"\" ... dagger provides a TriggerTaskTemplateBuilder helper to model the task in the DAG. The set_time_to_execute_lookup_key on this builder is used to define the key to lookup the trigger time provided in the runtime parameters of the task","title":"TriggerTask"},{"location":"usage-guide/fundamentals/#decisiontask","text":"This type of task is similar to the case..switch statement in a programming language. It returns the next task to execute based on the execution logic. A decision task needs to implement @abc . abstractmethod async def evaluate ( self , ** kwargs : Any ) -> Optional [ UUID ]: ... This method returns the UUID of the next task to execute in the execution path The Engine provides a DecisionTaskTemplateBuilder to model a decision task in the DAG","title":"DecisionTask"},{"location":"usage-guide/fundamentals/#monitoredtask","text":"dagger provides the IMonitoredTask interface which can be implemented on any task to provide a way to monitor that task. When the monitor is triggered based on a trigger, the process_monitored_task method is invoked on the MonitoringTask . Dagger comes built in with MonitoredProcessTemplateDAGInstance","title":"MonitoredTask"},{"location":"usage-guide/fundamentals/#parallelcompositetask","text":"SUB-DAG Task to execute parallel tasks and wait until all of them are in a terminal state before progressing to the next task This task can be embedded as a child of the root node or a process node","title":"ParallelCompositeTask"},{"location":"usage-guide/fundamentals/#intervaltask","text":"A type of Task to Trigger at a trigger time and execute multiple times until the execution completes. The task is retried until the timeout is reached periodically after the trigger time","title":"IntervalTask"},{"location":"usage-guide/fundamentals/#monitoringtask","text":"A Type of TriggerTask that executes at s specific time and checks on the monitored task to execute some domain specific logic","title":"MonitoringTask"},{"location":"usage-guide/fundamentals/#sensortask","text":"A type of task that halts execution of the workflow until a condition is met. When the condition is met the on_message method on this task is invoked","title":"SensorTask"},{"location":"usage-guide/fundamentals/#imonitoredtask","text":"An Abstract interface to enable monitoring of a task. Any Task that implements this interface will need to setup a MonitoringTask by implementing. Dagger ships with a default implementation using `` @abc . abstractmethod async def setup_monitoring_task ( self , workflow_instance : ITask ) -> None : ...","title":"IMonitoredTask"},{"location":"usage-guide/fundamentals/#restful-api","text":"The framework provides a RESTFul API to retrieve the status of root task instances. Root task is the instance created using the TaskTemplate which then has multiple, chained ProcessTasks and child tasks(KafkaCommand and KafkaListener tasks) h tt p : //<hostname>:6066/tasks/instances [ { \"child_dags\" : [], \"child_tasks\" : [ { \"child_dags\" : [ \"89bbf26c-0727-11ea-96e5-0242ac150004\" , \"89bc1486-0727-11ea-96e5-0242ac150004\" ], \"correlatable_key\" : null , \"id\" : \"89bbedd0-0727-11ea-96e5-0242ac150004\" , \"lastupdated\" : 1573767727 , \"parent_id\" : \"89bbe43e-0727-11ea-96e5-0242ac150004\" , \"process_name\" : \"PAYMENT\" , \"runtime_parameters\" : { \"order_number\" : \"ID000\" , \"customer\" : \"ID000\" }, \"status\" : { \"code\" : \"COMPLETED\" , \"value\" : \"Complete\" }, \"task_type\" : \"NON_ROOT\" , \"time_completed\" : 1573767727 , \"time_created\" : 1573767624 , \"time_submitted\" : 1573767698 }, { \"child_dags\" : [ \"89bc3984-0727-11ea-96e5-0242ac150004\" , \"89bc482a-0727-11ea-96e5-0242ac150004\" ], \"correlatable_key\" : null , \"id\" : \"89bc35f6-0727-11ea-96e5-0242ac150004\" , \"lastupdated\" : 1573767727 , \"parent_id\" : \"89bbe43e-0727-11ea-96e5-0242ac150004\" , \"process_name\" : \"SHIPPING\" , \"runtime_parameters\" : { \"order_number\" : \"ID000\" , \"customer\" : \"ID000\" }, \"status\" : { \"code\" : \"EXECUTING\" , \"value\" : \"Executing\" }, \"task_type\" : \"NON_ROOT\" , \"time_completed\" : 0 , \"time_created\" : 1573767624 , \"time_submitted\" : 1573767727 } ], \"correlatable_key\" : null , \"id\" : \"89bbe43e-0727-11ea-96e5-0242ac150004\" , \"lastupdated\" : 1573767624 , \"parent_id\" : null , \"runtime_parameters\" : { \"order_number\" : \"ID000\" , \"customer\" : \"ID000\" }, \"status\" : { \"code\" : \"EXECUTING\" , \"value\" : \"Executing\" }, \"task_type\" : \"ROOT\" , \"time_completed\" : 0 , \"time_created\" : 1573767624 , \"time_submitted\" : 1573767698 }]","title":"RESTful API"},{"location":"usage-guide/fundamentals/#detailed-sections","text":"getting-started","title":"Detailed Sections"}]}